{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohomghosh/FinCausal-2020/blob/master/RE_TRAIN_FinCausal_2020_best_solution_score_2022data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN0w7Po6YHgR"
      },
      "source": [
        "# Always run at the begining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBabedv_5nau",
        "outputId": "de9cb810-72b8-4746-e8da-fc82854042df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FinCausal-2020'...\n",
            "remote: Enumerating objects: 316, done.\u001b[K\n",
            "remote: Counting objects: 100% (316/316), done.\u001b[K\n",
            "remote: Compressing objects: 100% (240/240), done.\u001b[K\n",
            "remote: Total 316 (delta 92), reused 286 (delta 73), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (316/316), 1.55 MiB | 7.05 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/FinCausal-2020/\n",
        "!git clone https://github.com/sohomghosh/FinCausal-2020.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GcKbILfqPID",
        "outputId": "ba8b3137-e67b-4ae1-e7d8-ea4541e4761d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: certifi==2020.6.20 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 1)) (2020.6.20)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: dataclasses==0.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 4)) (0.6)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: future==0.18.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 6)) (0.18.2)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: joblib==0.16.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 9)) (0.16.0)\n",
            "Requirement already satisfied: Keras==2.4.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 10)) (2.4.3)\n",
            "Requirement already satisfied: mkl-service==2.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 12)) (3.5)\n",
            "Requirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 13)) (1.19.2)\n",
            "Requirement already satisfied: pandas==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 14)) (1.1.2)\n",
            "Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 15)) (3.13.0)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 16)) (2.8.1)\n",
            "Requirement already satisfied: pytz==2020.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 17)) (2020.1)\n",
            "Requirement already satisfied: PyYAML==5.3.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 18)) (5.3.1)\n",
            "Requirement already satisfied: regex==2020.7.14 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 19)) (2020.7.14)\n",
            "Requirement already satisfied: requests==2.24.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 20)) (2.24.0)\n",
            "Requirement already satisfied: sacremoses==0.0.43 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 21)) (0.0.43)\n",
            "Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 22)) (0.23.2)\n",
            "Requirement already satisfied: scipy==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 23)) (1.5.2)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 24)) (0.1.91)\n",
            "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 25)) (0.0.12)\n",
            "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 26)) (1.15.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 27)) (0.0)\n",
            "Requirement already satisfied: stanza==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 28)) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 29)) (2.1.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 30)) (0.8.1)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 31)) (1.6.0)\n",
            "Requirement already satisfied: tqdm==4.48.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 32)) (4.48.2)\n",
            "Requirement already satisfied: urllib3==1.25.10 in /usr/local/lib/python3.7/dist-packages (from -r /content/FinCausal-2020/requirements.txt (line 33)) (1.25.10)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (from mkl-service==2.3.0->-r /content/FinCausal-2020/requirements.txt (line 11)) (2019.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf==3.13.0->-r /content/FinCausal-2020/requirements.txt (line 15)) (57.4.0)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl->mkl-service==2.3.0->-r /content/FinCausal-2020/requirements.txt (line 11)) (2022.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/FinCausal-2020/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgX3jK1CDcCV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuxUPr8BrHYJ"
      },
      "outputs": [],
      "source": [
        "# Restart run-time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJBcCE1f8S9g",
        "outputId": "10853c3b-30fb-42a0-a4fb-fb8bebcd0be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 27.5MB/s]                    \n",
            "2022-04-16 12:54:33 INFO: Downloading default packages for language: en (English)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/default.zip: 100%|██████████| 428M/428M [01:16<00:00, 5.58MB/s]\n",
            "2022-04-16 12:55:57 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "stanza.download('en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcjkMARHCYc0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6Uv21OqBA3b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/FinCausal-2020/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMuNwwQv6Pyf"
      },
      "source": [
        "# [DONE] One time Only for training and test data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSbfAyakCPJK"
      },
      "outputs": [],
      "source": [
        "# Renamed data/pos.txt to data/pos_tags.txt\n",
        "! mv /content/FinCausal-2020/data/pos.txt /content/FinCausal-2020/data/pos_tags.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udmt7A3jxtDl",
        "outputId": "94670e74-053d-4651-d4b7-68c485e9fd36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-04-16 09:56:56--  https://raw.githubusercontent.com/sohomghosh/CEPN/main/FINCAUSAL_data_all_till2022_dedup_clean_formatted.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1395026 (1.3M) [text/plain]\n",
            "Saving to: ‘/content/FinCausal-2020/data/FINCAUSAL_data_all_till2022_dedup_clean_formatted.csv’\n",
            "\n",
            "FINCAUSAL_data_all_ 100%[===================>]   1.33M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-04-16 09:56:56 (25.4 MB/s) - ‘/content/FinCausal-2020/data/FINCAUSAL_data_all_till2022_dedup_clean_formatted.csv’ saved [1395026/1395026]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/sohomghosh/CEPN/main/FINCAUSAL_data_all_till2022_dedup_clean_formatted.csv -P /content/FinCausal-2020/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kySzGaPRzgQE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv('/content/FinCausal-2020/data/FINCAUSAL_data_all_till2022_dedup_clean_formatted.csv', sep = ';', header = None)\n",
        "train_data.columns = ['index','text',\t'cause',\t'effect',\t'cause_start',\t'effect_start']\n",
        "train_data.to_csv('/content/FinCausal-2020/data/train.csv', sep = ';', index = False)\n",
        "#This will overwrite the previous train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKWKDDmG12mF",
        "outputId": "358cac27-c7d3-45e3-9d47-82eaa801d4f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index;text;cause;effect;cause_start;effect_start\n",
            "0;The increase in net interest income was due primarily to a $152.9 billion increase in average outstanding loans, a $32.6 billion increase in average securities, partially offset by a 78 basis point decrease in earning asset yields.NIM was 3.22% for 2020, down 20 basis points compared to the prior year.;a $152.9 billion increase in average outstanding loans, a $32.6 billion increase in average securities, partially offset by a 78 basis point decrease in earning asset yields.;The increase in net interest income;57;0\n"
          ]
        }
      ],
      "source": [
        "!head -2 /content/FinCausal-2020/data/train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZjE6u6I2ElF",
        "outputId": "fd38024d-e7a2-4093-d2fd-263477fa8062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformation example:  {'sentence': 'Additional increases in noninterest income were primarily due to higher insurance income driven by improved production levels and acquisitions.', 'cause': 'higher insurance income driven by improved production levels and acquisitions.', 'effect': 'Additional increases in noninterest income'}\n",
            "100% 2808/2808 [08:00<00:00,  5.84it/s]\n"
          ]
        }
      ],
      "source": [
        "!python /content/FinCausal-2020/preprocess.py /content/FinCausal-2020/data/train.csv train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeELEB6_2-gO"
      },
      "outputs": [],
      "source": [
        "#train.txt has been generated for all data till 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gJsH3ye7IeE",
        "outputId": "f5149bd7-d6f7-4fc2-d346-ffdf7d29b31c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformation example:  {'sentence': ' Instead, the company reported itself as a family business, which is liable to a single 32% tax.'}\n",
            "100% 933/933 [02:20<00:00,  6.66it/s]\n"
          ]
        }
      ],
      "source": [
        "#blind text has been created from blind_fnp2022-fincausal-task2.csv by using proper formatting and encoding\n",
        "!python /content/FinCausal-2020/preprocess.py /content/FinCausal-2020/data/blind_test_2022.csv test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkgn9lef9GCS"
      },
      "outputs": [],
      "source": [
        "#test.txt has been generated for 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg00vp486uKE"
      },
      "source": [
        "# For Predictions only from pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G0hu2ItWgK3",
        "outputId": "513e7830-34b1-4d0e-baa7-74494f033ba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/FinCausal-2020/eval_bio.zip\n",
            "   creating: /content/FinCausal-2020/eval_bio/\n",
            "  inflating: /content/FinCausal-2020/eval_bio/test_predictions.txt  \n",
            "  inflating: /content/FinCausal-2020/eval_bio/.DS_Store  \n",
            "  inflating: /content/FinCausal-2020/__MACOSX/eval_bio/._.DS_Store  \n",
            "  inflating: /content/FinCausal-2020/eval_bio/tokenizer_config.json  \n",
            "  inflating: /content/FinCausal-2020/eval_bio/special_tokens_map.json  \n",
            "  inflating: /content/FinCausal-2020/eval_bio/config.json  \n",
            "  inflating: /content/FinCausal-2020/eval_bio/training_args.bin  \n",
            "  inflating: /content/FinCausal-2020/eval_bio/vocab.txt  \n",
            "  inflating: /content/FinCausal-2020/eval_bio/pytorch_model.bin  \n"
          ]
        }
      ],
      "source": [
        "# download and upload the file separately from https://drive.google.com/file/d/1omc-hy4uAb1JaeVrNQvbvGOQ3tZga7C3/view\n",
        "!unzip /content/FinCausal-2020/eval_bio.zip -d /content/FinCausal-2020/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZxN5FuN6YHb"
      },
      "source": [
        "# Train & Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F13D9smiCl15"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/FinCausal-2020/pred/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dDoWLjv7CkY"
      },
      "source": [
        "## Training BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXKYEeY9CX1o",
        "outputId": "d769b143-7dbf-428f-9ca7-a57ab54a03cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_labels: ['_', 'B-C', 'I-C', 'B-E', 'I-E']\n",
            "pos_labels: ['<pad>', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'GW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
            "Number of pos_labels: 50\n",
            "ADD_POS: False\n",
            "pos_labels: ['<pad>', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'GW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
            "Number of pos_labels: 50\n",
            "{\"learning_rate\": 2.6258309591642926e-05, \"loss\": 0.553298275668174, \"step\": 500}\n",
            "{\"learning_rate\": 2.51661918328585e-06, \"loss\": 0.2587094704005867, \"step\": 1000}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "04/16/2022 10:23:34 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "04/16/2022 10:23:34 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/16/2022 10:23:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='pred', overwrite_output_dir=False, add_pos=False, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir=None, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False)\n",
            "04/16/2022 10:23:34 - INFO - filelock -   Lock 139704458777232 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "04/16/2022 10:23:34 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp1qajue7s\n",
            "\rDownloading:   0%|          | 0.00/433 [00:00<?, ?B/s]\rDownloading: 100%|██████████| 433/433 [00:00<00:00, 320kB/s]\n",
            "04/16/2022 10:23:34 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "04/16/2022 10:23:34 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "04/16/2022 10:23:34 - INFO - filelock -   Lock 139704458777232 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "04/16/2022 10:23:34 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "04/16/2022 10:23:34 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"_\",\n",
            "    \"1\": \"B-C\",\n",
            "    \"2\": \"I-C\",\n",
            "    \"3\": \"B-E\",\n",
            "    \"4\": \"I-E\"\n",
            "  },\n",
            "  \"id2poslabel\": {\n",
            "    \"0\": \"<pad>\",\n",
            "    \"1\": \"$\",\n",
            "    \"2\": \"''\",\n",
            "    \"3\": \",\",\n",
            "    \"4\": \"-LRB-\",\n",
            "    \"5\": \"-RRB-\",\n",
            "    \"6\": \".\",\n",
            "    \"7\": \":\",\n",
            "    \"8\": \"ADD\",\n",
            "    \"9\": \"AFX\",\n",
            "    \"10\": \"CC\",\n",
            "    \"11\": \"CD\",\n",
            "    \"12\": \"DT\",\n",
            "    \"13\": \"EX\",\n",
            "    \"14\": \"FW\",\n",
            "    \"15\": \"GW\",\n",
            "    \"16\": \"HYPH\",\n",
            "    \"17\": \"IN\",\n",
            "    \"18\": \"JJ\",\n",
            "    \"19\": \"JJR\",\n",
            "    \"20\": \"JJS\",\n",
            "    \"21\": \"LS\",\n",
            "    \"22\": \"MD\",\n",
            "    \"23\": \"NFP\",\n",
            "    \"24\": \"NN\",\n",
            "    \"25\": \"NNP\",\n",
            "    \"26\": \"NNPS\",\n",
            "    \"27\": \"NNS\",\n",
            "    \"28\": \"PDT\",\n",
            "    \"29\": \"POS\",\n",
            "    \"30\": \"PRP\",\n",
            "    \"31\": \"PRP$\",\n",
            "    \"32\": \"RB\",\n",
            "    \"33\": \"RBR\",\n",
            "    \"34\": \"RBS\",\n",
            "    \"35\": \"RP\",\n",
            "    \"36\": \"SYM\",\n",
            "    \"37\": \"TO\",\n",
            "    \"38\": \"UH\",\n",
            "    \"39\": \"VB\",\n",
            "    \"40\": \"VBD\",\n",
            "    \"41\": \"VBG\",\n",
            "    \"42\": \"VBN\",\n",
            "    \"43\": \"VBP\",\n",
            "    \"44\": \"VBZ\",\n",
            "    \"45\": \"WDT\",\n",
            "    \"46\": \"WP\",\n",
            "    \"47\": \"WP$\",\n",
            "    \"48\": \"WRB\",\n",
            "    \"49\": \"``\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-C\": 1,\n",
            "    \"B-E\": 3,\n",
            "    \"I-C\": 2,\n",
            "    \"I-E\": 4,\n",
            "    \"_\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_pos_labels\": 50,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/16/2022 10:23:35 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "04/16/2022 10:23:35 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/16/2022 10:23:35 - INFO - filelock -   Lock 139704417842832 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "04/16/2022 10:23:35 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpnwz595jy\n",
            "\rDownloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\rDownloading:  90%|█████████ | 209k/232k [00:00<00:00, 1.77MB/s]\rDownloading: 100%|██████████| 232k/232k [00:00<00:00, 1.93MB/s]\n",
            "04/16/2022 10:23:35 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/16/2022 10:23:35 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/16/2022 10:23:35 - INFO - filelock -   Lock 139704417842832 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "04/16/2022 10:23:35 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/16/2022 10:23:35 - INFO - filelock -   Lock 139704417823440 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "04/16/2022 10:23:35 - INFO - transformers.file_utils -   https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpvhqqgjv_\n",
            "\rDownloading:   0%|          | 0.00/440M [00:00<?, ?B/s]\rDownloading:   1%|          | 2.87M/440M [00:00<00:15, 28.7MB/s]\rDownloading:   2%|▏         | 7.19M/440M [00:00<00:13, 31.9MB/s]\rDownloading:   3%|▎         | 11.4M/440M [00:00<00:12, 34.4MB/s]\rDownloading:   4%|▎         | 15.5M/440M [00:00<00:11, 36.2MB/s]\rDownloading:   4%|▍         | 19.8M/440M [00:00<00:11, 38.0MB/s]\rDownloading:   5%|▌         | 24.0M/440M [00:00<00:10, 39.0MB/s]\rDownloading:   6%|▋         | 27.9M/440M [00:00<00:10, 39.1MB/s]\rDownloading:   7%|▋         | 32.2M/440M [00:00<00:10, 40.3MB/s]\rDownloading:   8%|▊         | 36.5M/440M [00:00<00:09, 40.9MB/s]\rDownloading:   9%|▉         | 40.7M/440M [00:01<00:09, 41.3MB/s]\rDownloading:  10%|█         | 44.8M/440M [00:01<00:09, 41.3MB/s]\rDownloading:  11%|█         | 48.9M/440M [00:01<00:09, 40.8MB/s]\rDownloading:  12%|█▏        | 53.2M/440M [00:01<00:09, 41.2MB/s]\rDownloading:  13%|█▎        | 57.3M/440M [00:01<00:09, 40.9MB/s]\rDownloading:  14%|█▍        | 61.6M/440M [00:01<00:09, 41.5MB/s]\rDownloading:  15%|█▍        | 65.9M/440M [00:01<00:08, 41.9MB/s]\rDownloading:  16%|█▌        | 70.1M/440M [00:01<00:08, 42.1MB/s]\rDownloading:  17%|█▋        | 74.3M/440M [00:01<00:08, 41.4MB/s]\rDownloading:  18%|█▊        | 79.0M/440M [00:01<00:08, 42.8MB/s]\rDownloading:  19%|█▉        | 83.6M/440M [00:02<00:08, 43.7MB/s]\rDownloading:  20%|██        | 88.1M/440M [00:02<00:07, 44.2MB/s]\rDownloading:  21%|██        | 92.7M/440M [00:02<00:07, 44.5MB/s]\rDownloading:  22%|██▏       | 97.1M/440M [00:02<00:07, 44.6MB/s]\rDownloading:  23%|██▎       | 102M/440M [00:02<00:07, 44.2MB/s] \rDownloading:  24%|██▍       | 106M/440M [00:02<00:07, 44.6MB/s]\rDownloading:  25%|██▌       | 111M/440M [00:02<00:07, 44.6MB/s]\rDownloading:  26%|██▌       | 115M/440M [00:02<00:07, 41.1MB/s]\rDownloading:  27%|██▋       | 119M/440M [00:02<00:09, 34.9MB/s]\rDownloading:  28%|██▊       | 123M/440M [00:03<00:08, 36.2MB/s]\rDownloading:  29%|██▉       | 127M/440M [00:03<00:08, 37.2MB/s]\rDownloading:  30%|██▉       | 131M/440M [00:03<00:08, 38.1MB/s]\rDownloading:  31%|███       | 135M/440M [00:03<00:07, 38.7MB/s]\rDownloading:  32%|███▏      | 139M/440M [00:03<00:07, 38.7MB/s]\rDownloading:  33%|███▎      | 143M/440M [00:03<00:07, 39.2MB/s]\rDownloading:  33%|███▎      | 147M/440M [00:03<00:07, 39.4MB/s]\rDownloading:  34%|███▍      | 151M/440M [00:03<00:07, 39.3MB/s]\rDownloading:  35%|███▌      | 155M/440M [00:03<00:07, 39.1MB/s]\rDownloading:  36%|███▌      | 159M/440M [00:03<00:07, 36.9MB/s]\rDownloading:  37%|███▋      | 163M/440M [00:04<00:07, 37.3MB/s]\rDownloading:  38%|███▊      | 167M/440M [00:04<00:07, 38.7MB/s]\rDownloading:  39%|███▉      | 171M/440M [00:04<00:06, 39.8MB/s]\rDownloading:  40%|███▉      | 176M/440M [00:04<00:06, 40.6MB/s]\rDownloading:  41%|████      | 180M/440M [00:04<00:06, 38.9MB/s]\rDownloading:  42%|████▏     | 184M/440M [00:04<00:06, 40.3MB/s]\rDownloading:  43%|████▎     | 189M/440M [00:04<00:06, 41.1MB/s]\rDownloading:  44%|████▍     | 193M/440M [00:04<00:05, 41.8MB/s]\rDownloading:  45%|████▍     | 197M/440M [00:04<00:05, 42.4MB/s]\rDownloading:  46%|████▌     | 202M/440M [00:04<00:05, 41.7MB/s]\rDownloading:  47%|████▋     | 206M/440M [00:05<00:05, 42.5MB/s]\rDownloading:  48%|████▊     | 210M/440M [00:05<00:05, 42.9MB/s]\rDownloading:  49%|████▉     | 215M/440M [00:05<00:05, 43.5MB/s]\rDownloading:  50%|████▉     | 219M/440M [00:05<00:05, 38.3MB/s]\rDownloading:  51%|█████     | 224M/440M [00:05<00:05, 39.6MB/s]\rDownloading:  52%|█████▏    | 228M/440M [00:05<00:05, 40.5MB/s]\rDownloading:  53%|█████▎    | 232M/440M [00:05<00:05, 40.9MB/s]\rDownloading:  54%|█████▎    | 236M/440M [00:05<00:04, 41.5MB/s]\rDownloading:  55%|█████▍    | 241M/440M [00:05<00:04, 41.9MB/s]\rDownloading:  56%|█████▌    | 245M/440M [00:06<00:04, 40.8MB/s]\rDownloading:  57%|█████▋    | 249M/440M [00:06<00:04, 41.4MB/s]\rDownloading:  57%|█████▋    | 253M/440M [00:06<00:04, 41.0MB/s]\rDownloading:  58%|█████▊    | 257M/440M [00:06<00:04, 40.3MB/s]\rDownloading:  59%|█████▉    | 261M/440M [00:06<00:04, 39.4MB/s]\rDownloading:  60%|██████    | 266M/440M [00:06<00:04, 39.9MB/s]\rDownloading:  61%|██████    | 270M/440M [00:06<00:04, 40.1MB/s]\rDownloading:  62%|██████▏   | 274M/440M [00:06<00:04, 40.5MB/s]\rDownloading:  63%|██████▎   | 278M/440M [00:06<00:03, 41.3MB/s]\rDownloading:  64%|██████▍   | 282M/440M [00:06<00:03, 41.6MB/s]\rDownloading:  65%|██████▌   | 286M/440M [00:07<00:03, 41.1MB/s]\rDownloading:  66%|██████▌   | 291M/440M [00:07<00:03, 42.3MB/s]\rDownloading:  67%|██████▋   | 296M/440M [00:07<00:03, 43.5MB/s]\rDownloading:  68%|██████▊   | 300M/440M [00:07<00:03, 44.3MB/s]\rDownloading:  69%|██████▉   | 305M/440M [00:07<00:03, 44.4MB/s]\rDownloading:  70%|███████   | 309M/440M [00:07<00:02, 45.1MB/s]\rDownloading:  71%|███████▏  | 314M/440M [00:07<00:02, 45.3MB/s]\rDownloading:  72%|███████▏  | 319M/440M [00:07<00:02, 45.2MB/s]\rDownloading:  73%|███████▎  | 323M/440M [00:07<00:02, 45.1MB/s]\rDownloading:  74%|███████▍  | 328M/440M [00:07<00:02, 45.5MB/s]\rDownloading:  75%|███████▌  | 332M/440M [00:08<00:02, 44.2MB/s]\rDownloading:  76%|███████▋  | 337M/440M [00:08<00:02, 44.5MB/s]\rDownloading:  77%|███████▋  | 341M/440M [00:08<00:02, 44.8MB/s]\rDownloading:  79%|███████▊  | 346M/440M [00:08<00:02, 44.7MB/s]\rDownloading:  80%|███████▉  | 350M/440M [00:08<00:02, 44.8MB/s]\rDownloading:  81%|████████  | 355M/440M [00:08<00:01, 45.0MB/s]\rDownloading:  82%|████████▏ | 359M/440M [00:08<00:01, 44.5MB/s]\rDownloading:  83%|████████▎ | 364M/440M [00:08<00:01, 44.0MB/s]\rDownloading:  84%|████████▎ | 368M/440M [00:08<00:01, 44.0MB/s]\rDownloading:  85%|████████▍ | 373M/440M [00:09<00:02, 33.7MB/s]\rDownloading:  85%|████████▌ | 376M/440M [00:09<00:01, 34.3MB/s]\rDownloading:  86%|████████▋ | 381M/440M [00:09<00:01, 36.3MB/s]\rDownloading:  87%|████████▋ | 384M/440M [00:09<00:01, 36.6MB/s]\rDownloading:  88%|████████▊ | 389M/440M [00:09<00:01, 38.2MB/s]\rDownloading:  89%|████████▉ | 393M/440M [00:09<00:01, 39.4MB/s]\rDownloading:  90%|█████████ | 397M/440M [00:09<00:01, 40.1MB/s]\rDownloading:  91%|█████████ | 401M/440M [00:09<00:00, 40.5MB/s]\rDownloading:  92%|█████████▏| 405M/440M [00:09<00:00, 41.0MB/s]\rDownloading:  93%|█████████▎| 410M/440M [00:09<00:00, 40.4MB/s]\rDownloading:  94%|█████████▍| 414M/440M [00:10<00:00, 40.2MB/s]\rDownloading:  95%|█████████▍| 418M/440M [00:10<00:00, 38.6MB/s]\rDownloading:  96%|█████████▌| 422M/440M [00:10<00:00, 40.1MB/s]\rDownloading:  97%|█████████▋| 427M/440M [00:10<00:00, 41.2MB/s]\rDownloading:  98%|█████████▊| 431M/440M [00:10<00:00, 42.7MB/s]\rDownloading:  99%|█████████▉| 436M/440M [00:10<00:00, 43.8MB/s]\rDownloading: 100%|██████████| 440M/440M [00:10<00:00, 41.2MB/s]\n",
            "04/16/2022 10:23:46 - INFO - transformers.file_utils -   storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "04/16/2022 10:23:46 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "04/16/2022 10:23:46 - INFO - filelock -   Lock 139704417823440 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "04/16/2022 10:23:46 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "04/16/2022 10:23:51 - INFO - transformers.modeling_utils -   Weights of BertForTokenClassification not initialized from pretrained model: ['transition_', 'emission.weight', 'emission.bias']\n",
            "04/16/2022 10:23:51 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "04/16/2022 10:23:51 - INFO - utils -   Creating features from dataset file at data\n",
            "04/16/2022 10:23:51 - INFO - utils -   Writing example 0 of 2808\n",
            "04/16/2022 10:23:51 - INFO - utils -   *** Example ***\n",
            "04/16/2022 10:23:51 - INFO - utils -   guid: train-1\n",
            "04/16/2022 10:23:51 - INFO - utils -   tokens: [CLS] the increase in net interest income was due primarily to a $ 152 . 9 billion increase in average outstanding loans , a $ 32 . 6 billion increase in average securities , partially offset by a 78 basis point decrease in earning asset yields . ni ##m was 3 . 22 % for 2020 , down 20 basis points compared to the prior year . [SEP]\n",
            "04/16/2022 10:23:51 - INFO - utils -   input_ids: 101 1996 3623 1999 5658 3037 3318 2001 2349 3952 2000 1037 1002 15017 1012 1023 4551 3623 1999 2779 5151 10940 1010 1037 1002 3590 1012 1020 4551 3623 1999 2779 12012 1010 6822 16396 2011 1037 6275 3978 2391 9885 1999 7414 11412 16189 1012 9152 2213 2001 1017 1012 2570 1003 2005 12609 1010 2091 2322 3978 2685 4102 2000 1996 3188 2095 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   label_ids: -100 3 4 4 4 4 4 0 0 0 0 1 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 -100 0 0 -100 -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/16/2022 10:23:51 - INFO - utils -   pos_tag_ids: 0 12 24 17 18 24 24 40 17 32 17 12 1 11 0 0 11 24 17 18 18 27 3 12 1 11 0 0 11 24 17 18 27 3 32 42 17 12 11 24 24 24 17 41 24 27 6 25 0 40 11 0 0 24 17 11 3 17 11 24 27 42 17 12 18 24 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   *** Example ***\n",
            "04/16/2022 10:23:51 - INFO - utils -   guid: train-2\n",
            "04/16/2022 10:23:51 - INFO - utils -   tokens: [CLS] np ##as increased $ 70 ##3 million year over year , primarily due to pc ##i loans that would have been classified as non ##per ##form ##ing at december 31 , 2019 and loans exiting certain accommodation programs related to the cares act . non ##int ##eres ##t income increased $ 3 . 6 billion for the year with nearly all categories of non ##int ##eres ##t income being impacted by the merger . [SEP]\n",
            "04/16/2022 10:23:51 - INFO - utils -   input_ids: 101 27937 3022 3445 1002 3963 2509 2454 2095 2058 2095 1010 3952 2349 2000 7473 2072 10940 2008 2052 2031 2042 6219 2004 2512 4842 14192 2075 2012 2285 2861 1010 10476 1998 10940 22371 3056 11366 3454 3141 2000 1996 14977 2552 1012 2512 18447 18702 2102 3318 3445 1002 1017 1012 1020 4551 2005 1996 2095 2007 3053 2035 7236 1997 2512 18447 18702 2102 3318 2108 19209 2011 1996 7660 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   label_ids: -100 3 -100 4 4 4 -100 4 4 4 4 0 0 0 0 1 -100 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 0 -100 -100 -100 0 0 0 0 -100 -100 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/16/2022 10:23:51 - INFO - utils -   pos_tag_ids: 0 27 0 40 1 11 0 11 24 17 24 3 32 17 17 24 0 27 45 22 39 42 42 17 24 0 0 0 17 25 11 3 11 10 27 41 18 24 27 18 17 12 27 25 0 24 0 0 0 24 42 1 11 0 0 11 17 12 24 17 32 12 27 17 24 0 0 0 24 41 42 17 12 25 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   *** Example ***\n",
            "04/16/2022 10:23:51 - INFO - utils -   guid: train-3\n",
            "04/16/2022 10:23:51 - INFO - utils -   tokens: [CLS] additional increases in non ##int ##eres ##t income were primarily due to higher insurance income driven by improved production levels and acquisitions . [SEP]\n",
            "04/16/2022 10:23:51 - INFO - utils -   input_ids: 101 3176 7457 1999 2512 18447 18702 2102 3318 2020 3952 2349 2000 3020 5427 3318 5533 2011 5301 2537 3798 1998 19530 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   label_ids: -100 3 4 4 4 -100 -100 -100 4 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/16/2022 10:23:51 - INFO - utils -   pos_tag_ids: 0 18 27 17 24 0 0 0 24 40 32 17 17 19 24 24 42 17 42 24 27 10 27 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   *** Example ***\n",
            "04/16/2022 10:23:51 - INFO - utils -   guid: train-4\n",
            "04/16/2022 10:23:51 - INFO - utils -   tokens: [CLS] during 2020 , the company also issued $ 6 . 5 billion of senior and subordinate ##d long - term debt . total shareholders ' equity was $ 70 . 9 billion at december 31 , 2020 , up $ 4 . 4 billion compared to the prior year . the increase is due to net income in excess of divide ##nds paid of $ 1 . 8 billion and o ##ci of $ 1 . 6 billion . [SEP]\n",
            "04/16/2022 10:23:51 - INFO - utils -   input_ids: 101 2076 12609 1010 1996 2194 2036 3843 1002 1020 1012 1019 4551 1997 3026 1998 15144 2094 2146 1011 2744 7016 1012 2561 15337 1005 10067 2001 1002 3963 1012 1023 4551 2012 2285 2861 1010 12609 1010 2039 1002 1018 1012 1018 4551 4102 2000 1996 3188 2095 1012 1996 3623 2003 2349 2000 5658 3318 1999 9987 1997 11443 18376 3825 1997 1002 1015 1012 1022 4551 1998 1051 6895 1997 1002 1015 1012 1020 4551 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   label_ids: -100 3 4 4 4 4 4 4 4 4 -100 -100 4 4 4 4 4 -100 4 4 4 4 4 4 4 4 4 4 4 4 -100 -100 4 4 4 4 4 4 4 4 4 4 -100 -100 4 4 4 4 4 4 4 1 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 -100 -100 2 2 2 -100 2 2 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/16/2022 10:23:51 - INFO - utils -   pos_tag_ids: 0 17 11 3 12 24 32 40 1 11 0 0 11 17 18 10 42 0 18 16 24 24 6 18 27 29 24 40 1 11 0 0 11 17 25 11 3 11 3 17 1 11 0 0 11 42 17 12 18 24 6 12 24 44 17 17 18 24 17 24 17 27 0 42 17 1 11 0 0 11 10 24 0 17 1 11 0 0 11 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   *** Example ***\n",
            "04/16/2022 10:23:51 - INFO - utils -   guid: train-5\n",
            "04/16/2022 10:23:51 - INFO - utils -   tokens: [CLS] the significant increases in earnings assets and lia ##bilities are primarily due to the merger , as well as impacts from the co ##vid - 19 pan ##de ##mic and the resulting government stimulus programs . [SEP]\n",
            "04/16/2022 10:23:51 - INFO - utils -   input_ids: 101 1996 3278 7457 1999 16565 7045 1998 22393 14680 2024 3952 2349 2000 1996 7660 1010 2004 2092 2004 14670 2013 1996 2522 17258 1011 2539 6090 3207 7712 1998 1996 4525 2231 19220 3454 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:51 - INFO - utils -   label_ids: -100 3 4 4 4 4 4 4 4 -100 0 0 0 0 1 2 2 2 2 2 2 2 2 2 -100 2 2 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/16/2022 10:23:51 - INFO - utils -   pos_tag_ids: 0 12 18 27 17 27 27 10 27 0 43 32 17 17 12 25 3 32 32 17 27 17 12 25 0 16 11 24 0 0 10 12 41 24 24 27 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:23:59 - INFO - utils -   Saving features into cached file data/cached_train_BertTokenizer_350\n",
            "04/16/2022 10:24:01 - INFO - transformers.trainer -   You are instantiating a Trainer but wandb is not installed. Install it to use Weights & Biases logging.\n",
            "04/16/2022 10:24:03 - INFO - transformers.trainer -   ***** Running training *****\n",
            "04/16/2022 10:24:03 - INFO - transformers.trainer -     Num examples = 2808\n",
            "04/16/2022 10:24:03 - INFO - transformers.trainer -     Num Epochs = 3\n",
            "04/16/2022 10:24:03 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "04/16/2022 10:24:03 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "04/16/2022 10:24:03 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "04/16/2022 10:24:03 - INFO - transformers.trainer -     Total optimization steps = 1053\n",
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "\rIteration:   0%|          | 0/351 [00:00<?, ?it/s]\u001b[A/content/FinCausal-2020/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "\rIteration:   0%|          | 1/351 [00:01<06:27,  1.11s/it]\u001b[A\n",
            "\rIteration:   1%|          | 2/351 [00:02<06:14,  1.07s/it]\u001b[A\n",
            "\rIteration:   1%|          | 3/351 [00:03<06:03,  1.04s/it]\u001b[A\n",
            "\rIteration:   1%|          | 4/351 [00:04<05:54,  1.02s/it]\u001b[A\n",
            "\rIteration:   1%|▏         | 5/351 [00:05<05:47,  1.00s/it]\u001b[A\n",
            "\rIteration:   2%|▏         | 6/351 [00:05<05:44,  1.00it/s]\u001b[A\n",
            "\rIteration:   2%|▏         | 7/351 [00:06<05:41,  1.01it/s]\u001b[A\n",
            "\rIteration:   2%|▏         | 8/351 [00:07<05:36,  1.02it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 9/351 [00:08<05:33,  1.02it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 10/351 [00:09<05:34,  1.02it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 11/351 [00:10<05:33,  1.02it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 12/351 [00:11<05:30,  1.03it/s]\u001b[A\n",
            "\rIteration:   4%|▎         | 13/351 [00:12<05:31,  1.02it/s]\u001b[A\n",
            "\rIteration:   4%|▍         | 14/351 [00:13<05:31,  1.02it/s]\u001b[A\n",
            "\rIteration:   4%|▍         | 15/351 [00:14<05:28,  1.02it/s]\u001b[A\n",
            "\rIteration:   5%|▍         | 16/351 [00:15<05:24,  1.03it/s]\u001b[A\n",
            "\rIteration:   5%|▍         | 17/351 [00:16<05:21,  1.04it/s]\u001b[A\n",
            "\rIteration:   5%|▌         | 18/351 [00:17<05:20,  1.04it/s]\u001b[A\n",
            "\rIteration:   5%|▌         | 19/351 [00:18<05:27,  1.01it/s]\u001b[A\n",
            "\rIteration:   6%|▌         | 20/351 [00:19<05:34,  1.01s/it]\u001b[A\n",
            "\rIteration:   6%|▌         | 21/351 [00:20<05:27,  1.01it/s]\u001b[A\n",
            "\rIteration:   6%|▋         | 22/351 [00:21<05:23,  1.02it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 23/351 [00:22<05:20,  1.02it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 24/351 [00:23<05:19,  1.03it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 25/351 [00:24<05:17,  1.03it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 26/351 [00:25<05:16,  1.03it/s]\u001b[A\n",
            "\rIteration:   8%|▊         | 27/351 [00:26<05:16,  1.02it/s]\u001b[A\n",
            "\rIteration:   8%|▊         | 28/351 [00:27<05:14,  1.03it/s]\u001b[A\n",
            "\rIteration:   8%|▊         | 29/351 [00:28<05:13,  1.03it/s]\u001b[A\n",
            "\rIteration:   9%|▊         | 30/351 [00:29<05:12,  1.03it/s]\u001b[A\n",
            "\rIteration:   9%|▉         | 31/351 [00:30<05:13,  1.02it/s]\u001b[A\n",
            "\rIteration:   9%|▉         | 32/351 [00:31<05:12,  1.02it/s]\u001b[A\n",
            "\rIteration:   9%|▉         | 33/351 [00:32<05:09,  1.03it/s]\u001b[A\n",
            "\rIteration:  10%|▉         | 34/351 [00:33<05:09,  1.03it/s]\u001b[A\n",
            "\rIteration:  10%|▉         | 35/351 [00:34<05:08,  1.03it/s]\u001b[A\n",
            "\rIteration:  10%|█         | 36/351 [00:35<05:07,  1.03it/s]\u001b[A\n",
            "\rIteration:  11%|█         | 37/351 [00:36<05:06,  1.02it/s]\u001b[A\n",
            "\rIteration:  11%|█         | 38/351 [00:37<05:06,  1.02it/s]\u001b[A\n",
            "\rIteration:  11%|█         | 39/351 [00:38<05:04,  1.02it/s]\u001b[A\n",
            "\rIteration:  11%|█▏        | 40/351 [00:39<05:05,  1.02it/s]\u001b[A\n",
            "\rIteration:  12%|█▏        | 41/351 [00:40<05:03,  1.02it/s]\u001b[A\n",
            "\rIteration:  12%|█▏        | 42/351 [00:41<05:03,  1.02it/s]\u001b[A\n",
            "\rIteration:  12%|█▏        | 43/351 [00:42<05:02,  1.02it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 44/351 [00:43<05:00,  1.02it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 45/351 [00:44<05:00,  1.02it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 46/351 [00:45<04:59,  1.02it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 47/351 [00:46<04:58,  1.02it/s]\u001b[A\n",
            "\rIteration:  14%|█▎        | 48/351 [00:47<04:57,  1.02it/s]\u001b[A\n",
            "\rIteration:  14%|█▍        | 49/351 [00:48<04:55,  1.02it/s]\u001b[A\n",
            "\rIteration:  14%|█▍        | 50/351 [00:49<04:54,  1.02it/s]\u001b[A\n",
            "\rIteration:  15%|█▍        | 51/351 [00:50<04:54,  1.02it/s]\u001b[A\n",
            "\rIteration:  15%|█▍        | 52/351 [00:50<04:52,  1.02it/s]\u001b[A\n",
            "\rIteration:  15%|█▌        | 53/351 [00:51<04:52,  1.02it/s]\u001b[A\n",
            "\rIteration:  15%|█▌        | 54/351 [00:52<04:51,  1.02it/s]\u001b[A\n",
            "\rIteration:  16%|█▌        | 55/351 [00:53<04:51,  1.02it/s]\u001b[A\n",
            "\rIteration:  16%|█▌        | 56/351 [00:54<04:49,  1.02it/s]\u001b[A\n",
            "\rIteration:  16%|█▌        | 57/351 [00:55<04:50,  1.01it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 58/351 [00:56<04:48,  1.02it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 59/351 [00:57<04:47,  1.01it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 60/351 [00:58<04:46,  1.02it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 61/351 [00:59<04:46,  1.01it/s]\u001b[A\n",
            "\rIteration:  18%|█▊        | 62/351 [01:00<04:45,  1.01it/s]\u001b[A\n",
            "\rIteration:  18%|█▊        | 63/351 [01:01<04:43,  1.01it/s]\u001b[A\n",
            "\rIteration:  18%|█▊        | 64/351 [01:02<04:42,  1.02it/s]\u001b[A\n",
            "\rIteration:  19%|█▊        | 65/351 [01:03<04:42,  1.01it/s]\u001b[A\n",
            "\rIteration:  19%|█▉        | 66/351 [01:04<04:41,  1.01it/s]\u001b[A\n",
            "\rIteration:  19%|█▉        | 67/351 [01:05<04:39,  1.02it/s]\u001b[A\n",
            "\rIteration:  19%|█▉        | 68/351 [01:06<04:38,  1.02it/s]\u001b[A\n",
            "\rIteration:  20%|█▉        | 69/351 [01:07<04:37,  1.02it/s]\u001b[A\n",
            "\rIteration:  20%|█▉        | 70/351 [01:08<04:36,  1.02it/s]\u001b[A\n",
            "\rIteration:  20%|██        | 71/351 [01:09<04:35,  1.02it/s]\u001b[A\n",
            "\rIteration:  21%|██        | 72/351 [01:10<04:34,  1.02it/s]\u001b[A\n",
            "\rIteration:  21%|██        | 73/351 [01:11<04:33,  1.02it/s]\u001b[A\n",
            "\rIteration:  21%|██        | 74/351 [01:12<04:31,  1.02it/s]\u001b[A\n",
            "\rIteration:  21%|██▏       | 75/351 [01:13<04:31,  1.02it/s]\u001b[A\n",
            "\rIteration:  22%|██▏       | 76/351 [01:14<04:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  22%|██▏       | 77/351 [01:15<04:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  22%|██▏       | 78/351 [01:16<04:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 79/351 [01:17<04:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 80/351 [01:18<04:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 81/351 [01:19<04:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 82/351 [01:20<04:25,  1.01it/s]\u001b[A\n",
            "\rIteration:  24%|██▎       | 83/351 [01:21<04:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  24%|██▍       | 84/351 [01:22<04:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  24%|██▍       | 85/351 [01:23<04:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  25%|██▍       | 86/351 [01:24<04:23,  1.00it/s]\u001b[A\n",
            "\rIteration:  25%|██▍       | 87/351 [01:25<04:36,  1.05s/it]\u001b[A\n",
            "\rIteration:  25%|██▌       | 88/351 [01:26<04:30,  1.03s/it]\u001b[A\n",
            "\rIteration:  25%|██▌       | 89/351 [01:27<04:26,  1.02s/it]\u001b[A\n",
            "\rIteration:  26%|██▌       | 90/351 [01:28<04:23,  1.01s/it]\u001b[A\n",
            "\rIteration:  26%|██▌       | 91/351 [01:29<04:21,  1.01s/it]\u001b[A\n",
            "\rIteration:  26%|██▌       | 92/351 [01:30<04:20,  1.00s/it]\u001b[A\n",
            "\rIteration:  26%|██▋       | 93/351 [01:31<04:18,  1.00s/it]\u001b[A\n",
            "\rIteration:  27%|██▋       | 94/351 [01:32<04:15,  1.01it/s]\u001b[A\n",
            "\rIteration:  27%|██▋       | 95/351 [01:33<04:11,  1.02it/s]\u001b[A\n",
            "\rIteration:  27%|██▋       | 96/351 [01:34<04:09,  1.02it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 97/351 [01:35<04:07,  1.03it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 98/351 [01:36<04:05,  1.03it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 99/351 [01:37<04:05,  1.03it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 100/351 [01:38<04:04,  1.03it/s]\u001b[A\n",
            "\rIteration:  29%|██▉       | 101/351 [01:39<04:04,  1.02it/s]\u001b[A\n",
            "\rIteration:  29%|██▉       | 102/351 [01:40<04:04,  1.02it/s]\u001b[A\n",
            "\rIteration:  29%|██▉       | 103/351 [01:41<04:04,  1.02it/s]\u001b[A\n",
            "\rIteration:  30%|██▉       | 104/351 [01:42<04:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  30%|██▉       | 105/351 [01:43<04:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  30%|███       | 106/351 [01:44<04:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  30%|███       | 107/351 [01:45<04:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  31%|███       | 108/351 [01:46<04:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  31%|███       | 109/351 [01:47<03:58,  1.01it/s]\u001b[A\n",
            "\rIteration:  31%|███▏      | 110/351 [01:48<03:57,  1.01it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 111/351 [01:49<03:56,  1.02it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 112/351 [01:50<03:55,  1.01it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 113/351 [01:51<03:55,  1.01it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 114/351 [01:52<03:54,  1.01it/s]\u001b[A\n",
            "\rIteration:  33%|███▎      | 115/351 [01:53<03:53,  1.01it/s]\u001b[A\n",
            "\rIteration:  33%|███▎      | 116/351 [01:54<03:52,  1.01it/s]\u001b[A\n",
            "\rIteration:  33%|███▎      | 117/351 [01:55<03:52,  1.01it/s]\u001b[A\n",
            "\rIteration:  34%|███▎      | 118/351 [01:56<03:51,  1.01it/s]\u001b[A\n",
            "\rIteration:  34%|███▍      | 119/351 [01:57<03:50,  1.01it/s]\u001b[A\n",
            "\rIteration:  34%|███▍      | 120/351 [01:58<03:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  34%|███▍      | 121/351 [01:59<03:47,  1.01it/s]\u001b[A\n",
            "\rIteration:  35%|███▍      | 122/351 [02:00<03:47,  1.01it/s]\u001b[A\n",
            "\rIteration:  35%|███▌      | 123/351 [02:01<03:46,  1.01it/s]\u001b[A\n",
            "\rIteration:  35%|███▌      | 124/351 [02:02<03:44,  1.01it/s]\u001b[A\n",
            "\rIteration:  36%|███▌      | 125/351 [02:03<03:43,  1.01it/s]\u001b[A\n",
            "\rIteration:  36%|███▌      | 126/351 [02:04<03:42,  1.01it/s]\u001b[A\n",
            "\rIteration:  36%|███▌      | 127/351 [02:05<03:42,  1.01it/s]\u001b[A\n",
            "\rIteration:  36%|███▋      | 128/351 [02:06<03:41,  1.01it/s]\u001b[A\n",
            "\rIteration:  37%|███▋      | 129/351 [02:07<03:39,  1.01it/s]\u001b[A\n",
            "\rIteration:  37%|███▋      | 130/351 [02:08<03:38,  1.01it/s]\u001b[A\n",
            "\rIteration:  37%|███▋      | 131/351 [02:09<03:37,  1.01it/s]\u001b[A\n",
            "\rIteration:  38%|███▊      | 132/351 [02:10<03:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  38%|███▊      | 133/351 [02:11<03:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  38%|███▊      | 134/351 [02:12<03:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  38%|███▊      | 135/351 [02:13<03:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  39%|███▊      | 136/351 [02:14<03:31,  1.02it/s]\u001b[A\n",
            "\rIteration:  39%|███▉      | 137/351 [02:15<03:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  39%|███▉      | 138/351 [02:16<03:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  40%|███▉      | 139/351 [02:17<03:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  40%|███▉      | 140/351 [02:18<03:27,  1.02it/s]\u001b[A\n",
            "\rIteration:  40%|████      | 141/351 [02:18<03:26,  1.02it/s]\u001b[A\n",
            "\rIteration:  40%|████      | 142/351 [02:19<03:25,  1.02it/s]\u001b[A\n",
            "\rIteration:  41%|████      | 143/351 [02:20<03:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  41%|████      | 144/351 [02:21<03:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  41%|████▏     | 145/351 [02:22<03:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 146/351 [02:23<03:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 147/351 [02:24<03:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 148/351 [02:25<03:21,  1.01it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 149/351 [02:27<03:27,  1.03s/it]\u001b[A\n",
            "\rIteration:  43%|████▎     | 150/351 [02:28<03:23,  1.01s/it]\u001b[A\n",
            "\rIteration:  43%|████▎     | 151/351 [02:29<03:21,  1.01s/it]\u001b[A\n",
            "\rIteration:  43%|████▎     | 152/351 [02:29<03:19,  1.00s/it]\u001b[A\n",
            "\rIteration:  44%|████▎     | 153/351 [02:30<03:17,  1.00it/s]\u001b[A\n",
            "\rIteration:  44%|████▍     | 154/351 [02:32<03:19,  1.01s/it]\u001b[A\n",
            "\rIteration:  44%|████▍     | 155/351 [02:33<03:20,  1.02s/it]\u001b[A\n",
            "\rIteration:  44%|████▍     | 156/351 [02:34<03:17,  1.01s/it]\u001b[A\n",
            "\rIteration:  45%|████▍     | 157/351 [02:35<03:16,  1.01s/it]\u001b[A\n",
            "\rIteration:  45%|████▌     | 158/351 [02:36<03:13,  1.00s/it]\u001b[A\n",
            "\rIteration:  45%|████▌     | 159/351 [02:37<03:15,  1.02s/it]\u001b[A\n",
            "\rIteration:  46%|████▌     | 160/351 [02:38<03:15,  1.02s/it]\u001b[A\n",
            "\rIteration:  46%|████▌     | 161/351 [02:39<03:15,  1.03s/it]\u001b[A\n",
            "\rIteration:  46%|████▌     | 162/351 [02:40<03:15,  1.04s/it]\u001b[A\n",
            "\rIteration:  46%|████▋     | 163/351 [02:41<03:15,  1.04s/it]\u001b[A\n",
            "\rIteration:  47%|████▋     | 164/351 [02:42<03:14,  1.04s/it]\u001b[A\n",
            "\rIteration:  47%|████▋     | 165/351 [02:43<03:09,  1.02s/it]\u001b[A\n",
            "\rIteration:  47%|████▋     | 166/351 [02:44<03:06,  1.01s/it]\u001b[A\n",
            "\rIteration:  48%|████▊     | 167/351 [02:45<03:04,  1.00s/it]\u001b[A\n",
            "\rIteration:  48%|████▊     | 168/351 [02:46<03:02,  1.00it/s]\u001b[A\n",
            "\rIteration:  48%|████▊     | 169/351 [02:47<03:01,  1.00it/s]\u001b[A\n",
            "\rIteration:  48%|████▊     | 170/351 [02:48<02:59,  1.01it/s]\u001b[A\n",
            "\rIteration:  49%|████▊     | 171/351 [02:49<02:59,  1.00it/s]\u001b[A\n",
            "\rIteration:  49%|████▉     | 172/351 [02:50<02:57,  1.01it/s]\u001b[A\n",
            "\rIteration:  49%|████▉     | 173/351 [02:51<02:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  50%|████▉     | 174/351 [02:52<02:54,  1.01it/s]\u001b[A\n",
            "\rIteration:  50%|████▉     | 175/351 [02:53<02:53,  1.01it/s]\u001b[A\n",
            "\rIteration:  50%|█████     | 176/351 [02:54<02:52,  1.02it/s]\u001b[A\n",
            "\rIteration:  50%|█████     | 177/351 [02:55<02:49,  1.02it/s]\u001b[A\n",
            "\rIteration:  51%|█████     | 178/351 [02:56<02:48,  1.03it/s]\u001b[A\n",
            "\rIteration:  51%|█████     | 179/351 [02:57<02:48,  1.02it/s]\u001b[A\n",
            "\rIteration:  51%|█████▏    | 180/351 [02:58<03:02,  1.07s/it]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 181/351 [02:59<02:55,  1.03s/it]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 182/351 [03:00<02:58,  1.05s/it]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 183/351 [03:01<02:53,  1.03s/it]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 184/351 [03:02<02:49,  1.02s/it]\u001b[A\n",
            "\rIteration:  53%|█████▎    | 185/351 [03:03<02:47,  1.01s/it]\u001b[A\n",
            "\rIteration:  53%|█████▎    | 186/351 [03:04<02:44,  1.00it/s]\u001b[A\n",
            "\rIteration:  53%|█████▎    | 187/351 [03:05<02:43,  1.00it/s]\u001b[A\n",
            "\rIteration:  54%|█████▎    | 188/351 [03:06<02:44,  1.01s/it]\u001b[A\n",
            "\rIteration:  54%|█████▍    | 189/351 [03:07<02:50,  1.05s/it]\u001b[A\n",
            "\rIteration:  54%|█████▍    | 190/351 [03:08<02:46,  1.03s/it]\u001b[A\n",
            "\rIteration:  54%|█████▍    | 191/351 [03:09<02:43,  1.02s/it]\u001b[A\n",
            "\rIteration:  55%|█████▍    | 192/351 [03:10<02:44,  1.03s/it]\u001b[A\n",
            "\rIteration:  55%|█████▍    | 193/351 [03:11<02:41,  1.02s/it]\u001b[A\n",
            "\rIteration:  55%|█████▌    | 194/351 [03:12<02:40,  1.02s/it]\u001b[A\n",
            "\rIteration:  56%|█████▌    | 195/351 [03:13<02:42,  1.04s/it]\u001b[A\n",
            "\rIteration:  56%|█████▌    | 196/351 [03:14<02:39,  1.03s/it]\u001b[A\n",
            "\rIteration:  56%|█████▌    | 197/351 [03:15<02:38,  1.03s/it]\u001b[A\n",
            "\rIteration:  56%|█████▋    | 198/351 [03:16<02:37,  1.03s/it]\u001b[A\n",
            "\rIteration:  57%|█████▋    | 199/351 [03:17<02:34,  1.02s/it]\u001b[A\n",
            "\rIteration:  57%|█████▋    | 200/351 [03:18<02:32,  1.01s/it]\u001b[A\n",
            "\rIteration:  57%|█████▋    | 201/351 [03:19<02:30,  1.00s/it]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 202/351 [03:20<02:28,  1.01it/s]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 203/351 [03:21<02:28,  1.00s/it]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 204/351 [03:22<02:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 205/351 [03:23<02:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  59%|█████▊    | 206/351 [03:24<02:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  59%|█████▉    | 207/351 [03:25<02:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  59%|█████▉    | 208/351 [03:26<02:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|█████▉    | 209/351 [03:27<02:20,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|█████▉    | 210/351 [03:28<02:19,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|██████    | 211/351 [03:29<02:18,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|██████    | 212/351 [03:30<02:17,  1.01it/s]\u001b[A\n",
            "\rIteration:  61%|██████    | 213/351 [03:31<02:16,  1.01it/s]\u001b[A\n",
            "\rIteration:  61%|██████    | 214/351 [03:32<02:15,  1.01it/s]\u001b[A\n",
            "\rIteration:  61%|██████▏   | 215/351 [03:33<02:14,  1.01it/s]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 216/351 [03:34<02:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 217/351 [03:35<02:12,  1.01it/s]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 218/351 [03:36<02:13,  1.00s/it]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 219/351 [03:37<02:12,  1.00s/it]\u001b[A\n",
            "\rIteration:  63%|██████▎   | 220/351 [03:38<02:11,  1.00s/it]\u001b[A\n",
            "\rIteration:  63%|██████▎   | 221/351 [03:39<02:09,  1.00it/s]\u001b[A\n",
            "\rIteration:  63%|██████▎   | 222/351 [03:40<02:08,  1.00it/s]\u001b[A\n",
            "\rIteration:  64%|██████▎   | 223/351 [03:41<02:07,  1.00it/s]\u001b[A\n",
            "\rIteration:  64%|██████▍   | 224/351 [03:42<02:06,  1.01it/s]\u001b[A\n",
            "\rIteration:  64%|██████▍   | 225/351 [03:43<02:07,  1.01s/it]\u001b[A\n",
            "\rIteration:  64%|██████▍   | 226/351 [03:44<02:07,  1.02s/it]\u001b[A\n",
            "\rIteration:  65%|██████▍   | 227/351 [03:45<02:04,  1.00s/it]\u001b[A\n",
            "\rIteration:  65%|██████▍   | 228/351 [03:46<02:02,  1.00it/s]\u001b[A\n",
            "\rIteration:  65%|██████▌   | 229/351 [03:47<02:01,  1.00it/s]\u001b[A\n",
            "\rIteration:  66%|██████▌   | 230/351 [03:48<02:00,  1.00it/s]\u001b[A\n",
            "\rIteration:  66%|██████▌   | 231/351 [03:49<01:59,  1.01it/s]\u001b[A\n",
            "\rIteration:  66%|██████▌   | 232/351 [03:50<01:58,  1.01it/s]\u001b[A\n",
            "\rIteration:  66%|██████▋   | 233/351 [03:51<01:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  67%|██████▋   | 234/351 [03:52<01:55,  1.01it/s]\u001b[A\n",
            "\rIteration:  67%|██████▋   | 235/351 [03:53<01:54,  1.01it/s]\u001b[A\n",
            "\rIteration:  67%|██████▋   | 236/351 [03:54<01:53,  1.02it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 237/351 [03:55<01:52,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 238/351 [03:56<01:51,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 239/351 [03:57<01:50,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 240/351 [03:58<01:49,  1.02it/s]\u001b[A\n",
            "\rIteration:  69%|██████▊   | 241/351 [03:59<01:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  69%|██████▉   | 242/351 [04:00<01:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  69%|██████▉   | 243/351 [04:01<01:46,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|██████▉   | 244/351 [04:02<01:46,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|██████▉   | 245/351 [04:03<01:45,  1.00it/s]\u001b[A\n",
            "\rIteration:  70%|███████   | 246/351 [04:04<01:44,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|███████   | 247/351 [04:05<01:43,  1.01it/s]\u001b[A\n",
            "\rIteration:  71%|███████   | 248/351 [04:06<01:42,  1.01it/s]\u001b[A\n",
            "\rIteration:  71%|███████   | 249/351 [04:07<01:41,  1.01it/s]\u001b[A\n",
            "\rIteration:  71%|███████   | 250/351 [04:08<01:40,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 251/351 [04:09<01:38,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 252/351 [04:10<01:38,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 253/351 [04:11<01:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 254/351 [04:12<01:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  73%|███████▎  | 255/351 [04:13<01:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  73%|███████▎  | 256/351 [04:14<01:33,  1.02it/s]\u001b[A\n",
            "\rIteration:  73%|███████▎  | 257/351 [04:15<01:31,  1.02it/s]\u001b[A\n",
            "\rIteration:  74%|███████▎  | 258/351 [04:16<01:31,  1.02it/s]\u001b[A\n",
            "\rIteration:  74%|███████▍  | 259/351 [04:17<01:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  74%|███████▍  | 260/351 [04:18<01:31,  1.00s/it]\u001b[A\n",
            "\rIteration:  74%|███████▍  | 261/351 [04:19<01:30,  1.01s/it]\u001b[A\n",
            "\rIteration:  75%|███████▍  | 262/351 [04:20<01:30,  1.02s/it]\u001b[A\n",
            "\rIteration:  75%|███████▍  | 263/351 [04:21<01:28,  1.00s/it]\u001b[A\n",
            "\rIteration:  75%|███████▌  | 264/351 [04:22<01:26,  1.00it/s]\u001b[A\n",
            "\rIteration:  75%|███████▌  | 265/351 [04:23<01:27,  1.01s/it]\u001b[A\n",
            "\rIteration:  76%|███████▌  | 266/351 [04:24<01:24,  1.00it/s]\u001b[A\n",
            "\rIteration:  76%|███████▌  | 267/351 [04:25<01:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  76%|███████▋  | 268/351 [04:26<01:22,  1.00it/s]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 269/351 [04:27<01:22,  1.00s/it]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 270/351 [04:28<01:21,  1.01s/it]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 271/351 [04:29<01:20,  1.01s/it]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 272/351 [04:30<01:21,  1.04s/it]\u001b[A\n",
            "\rIteration:  78%|███████▊  | 273/351 [04:31<01:22,  1.06s/it]\u001b[A\n",
            "\rIteration:  78%|███████▊  | 274/351 [04:32<01:19,  1.04s/it]\u001b[A\n",
            "\rIteration:  78%|███████▊  | 275/351 [04:33<01:17,  1.02s/it]\u001b[A\n",
            "\rIteration:  79%|███████▊  | 276/351 [04:34<01:15,  1.01s/it]\u001b[A\n",
            "\rIteration:  79%|███████▉  | 277/351 [04:35<01:14,  1.00s/it]\u001b[A\n",
            "\rIteration:  79%|███████▉  | 278/351 [04:36<01:14,  1.01s/it]\u001b[A\n",
            "\rIteration:  79%|███████▉  | 279/351 [04:37<01:12,  1.01s/it]\u001b[A\n",
            "\rIteration:  80%|███████▉  | 280/351 [04:38<01:11,  1.00s/it]\u001b[A\n",
            "\rIteration:  80%|████████  | 281/351 [04:39<01:09,  1.00it/s]\u001b[A\n",
            "\rIteration:  80%|████████  | 282/351 [04:40<01:08,  1.00it/s]\u001b[A\n",
            "\rIteration:  81%|████████  | 283/351 [04:41<01:07,  1.01it/s]\u001b[A\n",
            "\rIteration:  81%|████████  | 284/351 [04:42<01:07,  1.01s/it]\u001b[A\n",
            "\rIteration:  81%|████████  | 285/351 [04:43<01:05,  1.00it/s]\u001b[A\n",
            "\rIteration:  81%|████████▏ | 286/351 [04:44<01:04,  1.01it/s]\u001b[A\n",
            "\rIteration:  82%|████████▏ | 287/351 [04:45<01:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  82%|████████▏ | 288/351 [04:46<01:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  82%|████████▏ | 289/351 [04:47<01:01,  1.01it/s]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 290/351 [04:48<01:01,  1.00s/it]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 291/351 [04:49<01:00,  1.00s/it]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 292/351 [04:50<00:59,  1.01s/it]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 293/351 [04:51<00:59,  1.02s/it]\u001b[A\n",
            "\rIteration:  84%|████████▍ | 294/351 [04:52<00:57,  1.01s/it]\u001b[A\n",
            "\rIteration:  84%|████████▍ | 295/351 [04:53<00:57,  1.02s/it]\u001b[A\n",
            "\rIteration:  84%|████████▍ | 296/351 [04:54<00:56,  1.02s/it]\u001b[A\n",
            "\rIteration:  85%|████████▍ | 297/351 [04:55<00:55,  1.03s/it]\u001b[A\n",
            "\rIteration:  85%|████████▍ | 298/351 [04:56<00:54,  1.03s/it]\u001b[A\n",
            "\rIteration:  85%|████████▌ | 299/351 [04:57<00:53,  1.02s/it]\u001b[A\n",
            "\rIteration:  85%|████████▌ | 300/351 [04:58<00:52,  1.03s/it]\u001b[A\n",
            "\rIteration:  86%|████████▌ | 301/351 [04:59<00:50,  1.02s/it]\u001b[A\n",
            "\rIteration:  86%|████████▌ | 302/351 [05:00<00:49,  1.01s/it]\u001b[A\n",
            "\rIteration:  86%|████████▋ | 303/351 [05:01<00:47,  1.00it/s]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 304/351 [05:02<00:46,  1.01it/s]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 305/351 [05:03<00:45,  1.01it/s]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 306/351 [05:04<00:44,  1.01it/s]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 307/351 [05:05<00:43,  1.01it/s]\u001b[A\n",
            "\rIteration:  88%|████████▊ | 308/351 [05:06<00:42,  1.01it/s]\u001b[A\n",
            "\rIteration:  88%|████████▊ | 309/351 [05:07<00:41,  1.01it/s]\u001b[A\n",
            "\rIteration:  88%|████████▊ | 310/351 [05:08<00:40,  1.01it/s]\u001b[A\n",
            "\rIteration:  89%|████████▊ | 311/351 [05:09<00:39,  1.01it/s]\u001b[A\n",
            "\rIteration:  89%|████████▉ | 312/351 [05:10<00:38,  1.01it/s]\u001b[A\n",
            "\rIteration:  89%|████████▉ | 313/351 [05:11<00:37,  1.01it/s]\u001b[A\n",
            "\rIteration:  89%|████████▉ | 314/351 [05:12<00:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  90%|████████▉ | 315/351 [05:13<00:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  90%|█████████ | 316/351 [05:14<00:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  90%|█████████ | 317/351 [05:15<00:33,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████ | 318/351 [05:16<00:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████ | 319/351 [05:17<00:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████ | 320/351 [05:18<00:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████▏| 321/351 [05:19<00:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  92%|█████████▏| 322/351 [05:20<00:28,  1.01it/s]\u001b[A\n",
            "\rIteration:  92%|█████████▏| 323/351 [05:21<00:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  92%|█████████▏| 324/351 [05:22<00:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 325/351 [05:23<00:25,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 326/351 [05:24<00:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 327/351 [05:25<00:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 328/351 [05:26<00:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  94%|█████████▎| 329/351 [05:27<00:21,  1.01it/s]\u001b[A\n",
            "\rIteration:  94%|█████████▍| 330/351 [05:28<00:20,  1.01it/s]\u001b[A\n",
            "\rIteration:  94%|█████████▍| 331/351 [05:29<00:19,  1.01it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▍| 332/351 [05:30<00:18,  1.01it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▍| 333/351 [05:31<00:17,  1.01it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▌| 334/351 [05:32<00:16,  1.02it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▌| 335/351 [05:33<00:15,  1.02it/s]\u001b[A\n",
            "\rIteration:  96%|█████████▌| 336/351 [05:34<00:14,  1.03it/s]\u001b[A\n",
            "\rIteration:  96%|█████████▌| 337/351 [05:35<00:13,  1.03it/s]\u001b[A\n",
            "\rIteration:  96%|█████████▋| 338/351 [05:36<00:12,  1.03it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 339/351 [05:37<00:11,  1.03it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 340/351 [05:38<00:10,  1.03it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 341/351 [05:39<00:09,  1.02it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 342/351 [05:40<00:08,  1.02it/s]\u001b[A\n",
            "\rIteration:  98%|█████████▊| 343/351 [05:41<00:07,  1.02it/s]\u001b[A\n",
            "\rIteration:  98%|█████████▊| 344/351 [05:42<00:06,  1.02it/s]\u001b[A\n",
            "\rIteration:  98%|█████████▊| 345/351 [05:43<00:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▊| 346/351 [05:44<00:04,  1.01it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▉| 347/351 [05:45<00:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▉| 348/351 [05:46<00:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▉| 349/351 [05:47<00:01,  1.01it/s]\u001b[A\n",
            "\rIteration: 100%|█████████▉| 350/351 [05:48<00:00,  1.01it/s]\u001b[A\n",
            "\rIteration: 100%|██████████| 351/351 [05:49<00:00,  1.01it/s]\u001b[A\rIteration: 100%|██████████| 351/351 [05:49<00:00,  1.01it/s]\n",
            "\rEpoch:  33%|███▎      | 1/3 [05:49<11:38, 349.02s/it]\n",
            "\rIteration:   0%|          | 0/351 [00:00<?, ?it/s]\u001b[A\n",
            "\rIteration:   0%|          | 1/351 [00:00<05:44,  1.01it/s]\u001b[A\n",
            "\rIteration:   1%|          | 2/351 [00:01<05:44,  1.01it/s]\u001b[A\n",
            "\rIteration:   1%|          | 3/351 [00:02<05:43,  1.01it/s]\u001b[A\n",
            "\rIteration:   1%|          | 4/351 [00:03<05:43,  1.01it/s]\u001b[A\n",
            "\rIteration:   1%|▏         | 5/351 [00:04<05:43,  1.01it/s]\u001b[A\n",
            "\rIteration:   2%|▏         | 6/351 [00:05<05:42,  1.01it/s]\u001b[A\n",
            "\rIteration:   2%|▏         | 7/351 [00:06<05:41,  1.01it/s]\u001b[A\n",
            "\rIteration:   2%|▏         | 8/351 [00:07<05:39,  1.01it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 9/351 [00:08<05:38,  1.01it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 10/351 [00:09<05:38,  1.01it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 11/351 [00:10<05:45,  1.02s/it]\u001b[A\n",
            "\rIteration:   3%|▎         | 12/351 [00:12<05:45,  1.02s/it]\u001b[A\n",
            "\rIteration:   4%|▎         | 13/351 [00:13<05:45,  1.02s/it]\u001b[A\n",
            "\rIteration:   4%|▍         | 14/351 [00:14<05:51,  1.04s/it]\u001b[A\n",
            "\rIteration:   4%|▍         | 15/351 [00:15<05:49,  1.04s/it]\u001b[A\n",
            "\rIteration:   5%|▍         | 16/351 [00:16<05:45,  1.03s/it]\u001b[A\n",
            "\rIteration:   5%|▍         | 17/351 [00:17<05:43,  1.03s/it]\u001b[A\n",
            "\rIteration:   5%|▌         | 18/351 [00:18<05:42,  1.03s/it]\u001b[A\n",
            "\rIteration:   5%|▌         | 19/351 [00:19<05:36,  1.01s/it]\u001b[A\n",
            "\rIteration:   6%|▌         | 20/351 [00:20<05:33,  1.01s/it]\u001b[A\n",
            "\rIteration:   6%|▌         | 21/351 [00:21<05:29,  1.00it/s]\u001b[A\n",
            "\rIteration:   6%|▋         | 22/351 [00:22<05:28,  1.00it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 23/351 [00:23<05:25,  1.01it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 24/351 [00:24<05:28,  1.00s/it]\u001b[A\n",
            "\rIteration:   7%|▋         | 25/351 [00:25<05:25,  1.00it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 26/351 [00:26<05:23,  1.00it/s]\u001b[A\n",
            "\rIteration:   8%|▊         | 27/351 [00:27<05:21,  1.01it/s]\u001b[A\n",
            "\rIteration:   8%|▊         | 28/351 [00:28<05:23,  1.00s/it]\u001b[A\n",
            "\rIteration:   8%|▊         | 29/351 [00:29<05:24,  1.01s/it]\u001b[A\n",
            "\rIteration:   9%|▊         | 30/351 [00:30<05:26,  1.02s/it]\u001b[A\n",
            "\rIteration:   9%|▉         | 31/351 [00:31<05:20,  1.00s/it]\u001b[A\n",
            "\rIteration:   9%|▉         | 32/351 [00:32<05:24,  1.02s/it]\u001b[A\n",
            "\rIteration:   9%|▉         | 33/351 [00:33<05:19,  1.00s/it]\u001b[A\n",
            "\rIteration:  10%|▉         | 34/351 [00:34<05:21,  1.02s/it]\u001b[A\n",
            "\rIteration:  10%|▉         | 35/351 [00:35<05:16,  1.00s/it]\u001b[A\n",
            "\rIteration:  10%|█         | 36/351 [00:36<05:14,  1.00it/s]\u001b[A\n",
            "\rIteration:  11%|█         | 37/351 [00:37<05:11,  1.01it/s]\u001b[A\n",
            "\rIteration:  11%|█         | 38/351 [00:38<05:11,  1.01it/s]\u001b[A\n",
            "\rIteration:  11%|█         | 39/351 [00:39<05:08,  1.01it/s]\u001b[A\n",
            "\rIteration:  11%|█▏        | 40/351 [00:40<05:08,  1.01it/s]\u001b[A\n",
            "\rIteration:  12%|█▏        | 41/351 [00:41<05:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  12%|█▏        | 42/351 [00:42<05:06,  1.01it/s]\u001b[A\n",
            "\rIteration:  12%|█▏        | 43/351 [00:43<05:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 44/351 [00:44<05:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 45/351 [00:45<05:01,  1.01it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 46/351 [00:46<05:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 47/351 [00:47<04:59,  1.01it/s]\u001b[A\n",
            "\rIteration:  14%|█▎        | 48/351 [00:48<04:59,  1.01it/s]\u001b[A\n",
            "\rIteration:  14%|█▍        | 49/351 [00:49<04:58,  1.01it/s]\u001b[A\n",
            "\rIteration:  14%|█▍        | 50/351 [00:50<04:56,  1.02it/s]\u001b[A\n",
            "\rIteration:  15%|█▍        | 51/351 [00:51<04:55,  1.02it/s]\u001b[A\n",
            "\rIteration:  15%|█▍        | 52/351 [00:51<04:54,  1.02it/s]\u001b[A\n",
            "\rIteration:  15%|█▌        | 53/351 [00:52<04:53,  1.02it/s]\u001b[A\n",
            "\rIteration:  15%|█▌        | 54/351 [00:53<04:52,  1.02it/s]\u001b[A\n",
            "\rIteration:  16%|█▌        | 55/351 [00:54<04:55,  1.00it/s]\u001b[A\n",
            "\rIteration:  16%|█▌        | 56/351 [00:55<04:52,  1.01it/s]\u001b[A\n",
            "\rIteration:  16%|█▌        | 57/351 [00:56<04:51,  1.01it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 58/351 [00:57<04:50,  1.01it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 59/351 [00:58<04:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 60/351 [00:59<04:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 61/351 [01:00<04:47,  1.01it/s]\u001b[A\n",
            "\rIteration:  18%|█▊        | 62/351 [01:01<04:45,  1.01it/s]\u001b[A\n",
            "\rIteration:  18%|█▊        | 63/351 [01:02<04:46,  1.01it/s]\u001b[A\n",
            "\rIteration:  18%|█▊        | 64/351 [01:03<04:46,  1.00it/s]\u001b[A\n",
            "\rIteration:  19%|█▊        | 65/351 [01:04<04:43,  1.01it/s]\u001b[A\n",
            "\rIteration:  19%|█▉        | 66/351 [01:05<04:40,  1.02it/s]\u001b[A\n",
            "\rIteration:  19%|█▉        | 67/351 [01:06<04:38,  1.02it/s]\u001b[A\n",
            "\rIteration:  19%|█▉        | 68/351 [01:07<04:38,  1.02it/s]\u001b[A\n",
            "\rIteration:  20%|█▉        | 69/351 [01:08<04:39,  1.01it/s]\u001b[A\n",
            "\rIteration:  20%|█▉        | 70/351 [01:09<04:37,  1.01it/s]\u001b[A\n",
            "\rIteration:  20%|██        | 71/351 [01:10<04:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  21%|██        | 72/351 [01:11<04:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  21%|██        | 73/351 [01:12<04:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  21%|██        | 74/351 [01:13<04:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  21%|██▏       | 75/351 [01:14<04:33,  1.01it/s]\u001b[A\n",
            "\rIteration:  22%|██▏       | 76/351 [01:15<04:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  22%|██▏       | 77/351 [01:16<04:29,  1.02it/s]\u001b[A\n",
            "\rIteration:  22%|██▏       | 78/351 [01:17<04:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 79/351 [01:18<04:28,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 80/351 [01:19<04:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 81/351 [01:20<04:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 82/351 [01:21<04:24,  1.02it/s]\u001b[A\n",
            "\rIteration:  24%|██▎       | 83/351 [01:22<04:23,  1.02it/s]\u001b[A\n",
            "\rIteration:  24%|██▍       | 84/351 [01:23<04:22,  1.02it/s]\u001b[A\n",
            "\rIteration:  24%|██▍       | 85/351 [01:24<04:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  25%|██▍       | 86/351 [01:25<04:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  25%|██▍       | 87/351 [01:26<04:20,  1.01it/s]\u001b[A\n",
            "\rIteration:  25%|██▌       | 88/351 [01:27<04:18,  1.02it/s]\u001b[A\n",
            "\rIteration:  25%|██▌       | 89/351 [01:28<04:17,  1.02it/s]\u001b[A\n",
            "\rIteration:  26%|██▌       | 90/351 [01:29<04:18,  1.01it/s]\u001b[A\n",
            "\rIteration:  26%|██▌       | 91/351 [01:30<04:16,  1.01it/s]\u001b[A\n",
            "\rIteration:  26%|██▌       | 92/351 [01:31<04:16,  1.01it/s]\u001b[A\n",
            "\rIteration:  26%|██▋       | 93/351 [01:32<04:14,  1.01it/s]\u001b[A\n",
            "\rIteration:  27%|██▋       | 94/351 [01:33<04:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  27%|██▋       | 95/351 [01:34<04:14,  1.01it/s]\u001b[A\n",
            "\rIteration:  27%|██▋       | 96/351 [01:35<04:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 97/351 [01:36<04:11,  1.01it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 98/351 [01:37<04:11,  1.01it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 99/351 [01:38<04:10,  1.01it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 100/351 [01:39<04:09,  1.01it/s]\u001b[A\n",
            "\rIteration:  29%|██▉       | 101/351 [01:40<04:08,  1.01it/s]\u001b[A\n",
            "\rIteration:  29%|██▉       | 102/351 [01:41<04:06,  1.01it/s]\u001b[A\n",
            "\rIteration:  29%|██▉       | 103/351 [01:42<04:06,  1.01it/s]\u001b[A\n",
            "\rIteration:  30%|██▉       | 104/351 [01:43<04:04,  1.01it/s]\u001b[A\n",
            "\rIteration:  30%|██▉       | 105/351 [01:44<04:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  30%|███       | 106/351 [01:45<04:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  30%|███       | 107/351 [01:46<04:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  31%|███       | 108/351 [01:47<04:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  31%|███       | 109/351 [01:48<03:59,  1.01it/s]\u001b[A\n",
            "\rIteration:  31%|███▏      | 110/351 [01:49<03:57,  1.01it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 111/351 [01:50<03:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 112/351 [01:51<03:55,  1.02it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 113/351 [01:52<03:54,  1.01it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 114/351 [01:53<03:53,  1.02it/s]\u001b[A\n",
            "\rIteration:  33%|███▎      | 115/351 [01:54<03:52,  1.02it/s]\u001b[A\n",
            "\rIteration:  33%|███▎      | 116/351 [01:55<03:51,  1.02it/s]\u001b[A\n",
            "\rIteration:  33%|███▎      | 117/351 [01:56<03:50,  1.01it/s]\u001b[A\n",
            "\rIteration:  34%|███▎      | 118/351 [01:57<03:49,  1.02it/s]\u001b[A\n",
            "\rIteration:  34%|███▍      | 119/351 [01:58<03:48,  1.02it/s]\u001b[A\n",
            "\rIteration:  34%|███▍      | 120/351 [01:59<03:47,  1.01it/s]\u001b[A\n",
            "\rIteration:  34%|███▍      | 121/351 [02:00<03:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  35%|███▍      | 122/351 [02:01<03:51,  1.01s/it]\u001b[A\n",
            "\rIteration:  35%|███▌      | 123/351 [02:02<03:53,  1.02s/it]\u001b[A\n",
            "\rIteration:  35%|███▌      | 124/351 [02:03<03:50,  1.01s/it]\u001b[A\n",
            "\rIteration:  36%|███▌      | 125/351 [02:04<03:48,  1.01s/it]\u001b[A\n",
            "\rIteration:  36%|███▌      | 126/351 [02:05<03:49,  1.02s/it]\u001b[A\n",
            "\rIteration:  36%|███▌      | 127/351 [02:06<03:57,  1.06s/it]\u001b[A\n",
            "\rIteration:  36%|███▋      | 128/351 [02:07<03:52,  1.04s/it]\u001b[A\n",
            "\rIteration:  37%|███▋      | 129/351 [02:08<03:47,  1.03s/it]\u001b[A\n",
            "\rIteration:  37%|███▋      | 130/351 [02:09<03:43,  1.01s/it]\u001b[A\n",
            "\rIteration:  37%|███▋      | 131/351 [02:10<03:41,  1.00s/it]\u001b[A\n",
            "\rIteration:  38%|███▊      | 132/351 [02:11<03:39,  1.00s/it]\u001b[A\n",
            "\rIteration:  38%|███▊      | 133/351 [02:12<03:37,  1.00it/s]\u001b[A\n",
            "\rIteration:  38%|███▊      | 134/351 [02:13<03:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  38%|███▊      | 135/351 [02:14<03:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  39%|███▊      | 136/351 [02:15<03:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  39%|███▉      | 137/351 [02:16<03:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  39%|███▉      | 138/351 [02:17<03:29,  1.02it/s]\u001b[A\n",
            "\rIteration:  40%|███▉      | 139/351 [02:18<03:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  40%|███▉      | 140/351 [02:19<03:28,  1.01it/s]\u001b[A\n",
            "\rIteration:  40%|████      | 141/351 [02:20<03:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  40%|████      | 142/351 [02:21<03:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  41%|████      | 143/351 [02:22<03:25,  1.01it/s]\u001b[A\n",
            "\rIteration:  41%|████      | 144/351 [02:23<03:23,  1.02it/s]\u001b[A\n",
            "\rIteration:  41%|████▏     | 145/351 [02:24<03:21,  1.02it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 146/351 [02:25<03:19,  1.03it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 147/351 [02:26<03:19,  1.02it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 148/351 [02:27<03:19,  1.02it/s]\u001b[A\r                                                     \r\n",
            "\r                                                            \r\u001b[A\rEpoch:  33%|███▎      | 1/3 [08:17<11:38, 349.02s/it]\n",
            "\rIteration:  42%|████▏     | 148/351 [02:28<03:19,  1.02it/s]\u001b[A04/16/2022 10:32:20 - INFO - transformers.trainer -   Saving model checkpoint to pred/checkpoint-500\n",
            "04/16/2022 10:32:20 - INFO - transformers.configuration_utils -   Configuration saved in pred/checkpoint-500/config.json\n",
            "04/16/2022 10:32:22 - INFO - transformers.modeling_utils -   Model weights saved in pred/checkpoint-500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "04/16/2022 10:32:25 - INFO - transformers.trainer -   Saving optimizer and scheduler states to pred/checkpoint-500\n",
            "\n",
            "\rIteration:  42%|████▏     | 149/351 [02:33<08:34,  2.54s/it]\u001b[A\n",
            "\rIteration:  43%|████▎     | 150/351 [02:34<07:03,  2.11s/it]\u001b[A\n",
            "\rIteration:  43%|████▎     | 151/351 [02:35<05:56,  1.78s/it]\u001b[A\n",
            "\rIteration:  43%|████▎     | 152/351 [02:36<05:07,  1.55s/it]\u001b[A\n",
            "\rIteration:  44%|████▎     | 153/351 [02:37<04:32,  1.38s/it]\u001b[A\n",
            "\rIteration:  44%|████▍     | 154/351 [02:38<04:08,  1.26s/it]\u001b[A\n",
            "\rIteration:  44%|████▍     | 155/351 [02:39<03:50,  1.17s/it]\u001b[A\n",
            "\rIteration:  44%|████▍     | 156/351 [02:40<03:37,  1.12s/it]\u001b[A\n",
            "\rIteration:  45%|████▍     | 157/351 [02:41<03:28,  1.08s/it]\u001b[A\n",
            "\rIteration:  45%|████▌     | 158/351 [02:42<03:23,  1.05s/it]\u001b[A\n",
            "\rIteration:  45%|████▌     | 159/351 [02:43<03:18,  1.03s/it]\u001b[A\n",
            "\rIteration:  46%|████▌     | 160/351 [02:44<03:14,  1.02s/it]\u001b[A\n",
            "\rIteration:  46%|████▌     | 161/351 [02:45<03:11,  1.01s/it]\u001b[A\n",
            "\rIteration:  46%|████▌     | 162/351 [02:46<03:08,  1.00it/s]\u001b[A\n",
            "\rIteration:  46%|████▋     | 163/351 [02:47<03:07,  1.00it/s]\u001b[A\n",
            "\rIteration:  47%|████▋     | 164/351 [02:48<03:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  47%|████▋     | 165/351 [02:49<03:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  47%|████▋     | 166/351 [02:50<03:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  48%|████▊     | 167/351 [02:51<03:01,  1.01it/s]\u001b[A\n",
            "\rIteration:  48%|████▊     | 168/351 [02:52<03:01,  1.01it/s]\u001b[A\n",
            "\rIteration:  48%|████▊     | 169/351 [02:53<03:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  48%|████▊     | 170/351 [02:54<02:58,  1.01it/s]\u001b[A\n",
            "\rIteration:  49%|████▊     | 171/351 [02:55<02:58,  1.01it/s]\u001b[A\n",
            "\rIteration:  49%|████▉     | 172/351 [02:56<02:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  49%|████▉     | 173/351 [02:57<02:55,  1.01it/s]\u001b[A\n",
            "\rIteration:  50%|████▉     | 174/351 [02:58<02:55,  1.01it/s]\u001b[A\n",
            "\rIteration:  50%|████▉     | 175/351 [02:59<02:54,  1.01it/s]\u001b[A\n",
            "\rIteration:  50%|█████     | 176/351 [03:00<02:52,  1.01it/s]\u001b[A\n",
            "\rIteration:  50%|█████     | 177/351 [03:01<02:52,  1.01it/s]\u001b[A\n",
            "\rIteration:  51%|█████     | 178/351 [03:02<02:51,  1.01it/s]\u001b[A\n",
            "\rIteration:  51%|█████     | 179/351 [03:03<02:50,  1.01it/s]\u001b[A\n",
            "\rIteration:  51%|█████▏    | 180/351 [03:04<02:49,  1.01it/s]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 181/351 [03:05<02:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 182/351 [03:06<02:47,  1.01it/s]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 183/351 [03:07<02:45,  1.01it/s]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 184/351 [03:08<02:44,  1.01it/s]\u001b[A\n",
            "\rIteration:  53%|█████▎    | 185/351 [03:09<02:44,  1.01it/s]\u001b[A\n",
            "\rIteration:  53%|█████▎    | 186/351 [03:10<02:43,  1.01it/s]\u001b[A\n",
            "\rIteration:  53%|█████▎    | 187/351 [03:11<02:42,  1.01it/s]\u001b[A\n",
            "\rIteration:  54%|█████▎    | 188/351 [03:12<02:41,  1.01it/s]\u001b[A\n",
            "\rIteration:  54%|█████▍    | 189/351 [03:13<02:39,  1.01it/s]\u001b[A\n",
            "\rIteration:  54%|█████▍    | 190/351 [03:14<02:38,  1.01it/s]\u001b[A\n",
            "\rIteration:  54%|█████▍    | 191/351 [03:14<02:37,  1.02it/s]\u001b[A\n",
            "\rIteration:  55%|█████▍    | 192/351 [03:15<02:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  55%|█████▍    | 193/351 [03:16<02:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  55%|█████▌    | 194/351 [03:17<02:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  56%|█████▌    | 195/351 [03:18<02:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  56%|█████▌    | 196/351 [03:19<02:33,  1.01it/s]\u001b[A\n",
            "\rIteration:  56%|█████▌    | 197/351 [03:20<02:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  56%|█████▋    | 198/351 [03:21<02:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  57%|█████▋    | 199/351 [03:22<02:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  57%|█████▋    | 200/351 [03:23<02:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  57%|█████▋    | 201/351 [03:24<02:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 202/351 [03:25<02:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 203/351 [03:26<02:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 204/351 [03:27<02:25,  1.01it/s]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 205/351 [03:28<02:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  59%|█████▊    | 206/351 [03:29<02:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  59%|█████▉    | 207/351 [03:30<02:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  59%|█████▉    | 208/351 [03:31<02:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|█████▉    | 209/351 [03:32<02:21,  1.00it/s]\u001b[A\n",
            "\rIteration:  60%|█████▉    | 210/351 [03:33<02:20,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|██████    | 211/351 [03:34<02:19,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|██████    | 212/351 [03:35<02:19,  1.00s/it]\u001b[A\n",
            "\rIteration:  61%|██████    | 213/351 [03:36<02:18,  1.00s/it]\u001b[A\n",
            "\rIteration:  61%|██████    | 214/351 [03:37<02:16,  1.00it/s]\u001b[A\n",
            "\rIteration:  61%|██████▏   | 215/351 [03:38<02:14,  1.01it/s]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 216/351 [03:39<02:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 217/351 [03:40<02:12,  1.01it/s]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 218/351 [03:41<02:09,  1.02it/s]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 219/351 [03:42<02:08,  1.03it/s]\u001b[A\n",
            "\rIteration:  63%|██████▎   | 220/351 [03:43<02:08,  1.02it/s]\u001b[A\n",
            "\rIteration:  63%|██████▎   | 221/351 [03:44<02:08,  1.01it/s]\u001b[A\n",
            "\rIteration:  63%|██████▎   | 222/351 [03:45<02:07,  1.01it/s]\u001b[A\n",
            "\rIteration:  64%|██████▎   | 223/351 [03:46<02:06,  1.02it/s]\u001b[A\n",
            "\rIteration:  64%|██████▍   | 224/351 [03:47<02:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  64%|██████▍   | 225/351 [03:48<02:04,  1.01it/s]\u001b[A\n",
            "\rIteration:  64%|██████▍   | 226/351 [03:49<02:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  65%|██████▍   | 227/351 [03:50<02:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  65%|██████▍   | 228/351 [03:51<02:01,  1.01it/s]\u001b[A\n",
            "\rIteration:  65%|██████▌   | 229/351 [03:52<02:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  66%|██████▌   | 230/351 [03:53<02:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  66%|██████▌   | 231/351 [03:54<01:58,  1.01it/s]\u001b[A\n",
            "\rIteration:  66%|██████▌   | 232/351 [03:55<01:57,  1.01it/s]\u001b[A\n",
            "\rIteration:  66%|██████▋   | 233/351 [03:56<01:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  67%|██████▋   | 234/351 [03:57<01:55,  1.01it/s]\u001b[A\n",
            "\rIteration:  67%|██████▋   | 235/351 [03:58<01:54,  1.01it/s]\u001b[A\n",
            "\rIteration:  67%|██████▋   | 236/351 [03:59<01:53,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 237/351 [04:00<01:53,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 238/351 [04:01<01:51,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 239/351 [04:02<01:50,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 240/351 [04:03<01:49,  1.02it/s]\u001b[A\n",
            "\rIteration:  69%|██████▊   | 241/351 [04:04<01:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  69%|██████▉   | 242/351 [04:05<01:47,  1.01it/s]\u001b[A\n",
            "\rIteration:  69%|██████▉   | 243/351 [04:06<01:46,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|██████▉   | 244/351 [04:07<01:45,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|██████▉   | 245/351 [04:08<01:44,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|███████   | 246/351 [04:09<01:43,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|███████   | 247/351 [04:10<01:42,  1.01it/s]\u001b[A\n",
            "\rIteration:  71%|███████   | 248/351 [04:11<01:41,  1.01it/s]\u001b[A\n",
            "\rIteration:  71%|███████   | 249/351 [04:12<01:40,  1.02it/s]\u001b[A\n",
            "\rIteration:  71%|███████   | 250/351 [04:13<01:39,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 251/351 [04:14<01:38,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 252/351 [04:15<01:37,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 253/351 [04:16<01:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 254/351 [04:17<01:35,  1.02it/s]\u001b[A\n",
            "\rIteration:  73%|███████▎  | 255/351 [04:18<01:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  73%|███████▎  | 256/351 [04:19<01:33,  1.02it/s]\u001b[A\n",
            "\rIteration:  73%|███████▎  | 257/351 [04:20<01:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  74%|███████▎  | 258/351 [04:21<01:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  74%|███████▍  | 259/351 [04:22<01:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  74%|███████▍  | 260/351 [04:23<01:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  74%|███████▍  | 261/351 [04:24<01:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  75%|███████▍  | 262/351 [04:25<01:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  75%|███████▍  | 263/351 [04:26<01:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  75%|███████▌  | 264/351 [04:27<01:25,  1.01it/s]\u001b[A\n",
            "\rIteration:  75%|███████▌  | 265/351 [04:28<01:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  76%|███████▌  | 266/351 [04:29<01:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  76%|███████▌  | 267/351 [04:30<01:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  76%|███████▋  | 268/351 [04:31<01:21,  1.01it/s]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 269/351 [04:32<01:21,  1.01it/s]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 270/351 [04:33<01:19,  1.01it/s]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 271/351 [04:34<01:18,  1.01it/s]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 272/351 [04:35<01:17,  1.01it/s]\u001b[A\n",
            "\rIteration:  78%|███████▊  | 273/351 [04:36<01:16,  1.02it/s]\u001b[A\n",
            "\rIteration:  78%|███████▊  | 274/351 [04:37<01:16,  1.01it/s]\u001b[A\n",
            "\rIteration:  78%|███████▊  | 275/351 [04:37<01:14,  1.01it/s]\u001b[A\n",
            "\rIteration:  79%|███████▊  | 276/351 [04:38<01:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  79%|███████▉  | 277/351 [04:39<01:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  79%|███████▉  | 278/351 [04:40<01:12,  1.01it/s]\u001b[A\n",
            "\rIteration:  79%|███████▉  | 279/351 [04:41<01:11,  1.01it/s]\u001b[A\n",
            "\rIteration:  80%|███████▉  | 280/351 [04:42<01:10,  1.01it/s]\u001b[A\n",
            "\rIteration:  80%|████████  | 281/351 [04:43<01:09,  1.01it/s]\u001b[A\n",
            "\rIteration:  80%|████████  | 282/351 [04:44<01:08,  1.01it/s]\u001b[A\n",
            "\rIteration:  81%|████████  | 283/351 [04:45<01:07,  1.01it/s]\u001b[A\n",
            "\rIteration:  81%|████████  | 284/351 [04:46<01:06,  1.01it/s]\u001b[A\n",
            "\rIteration:  81%|████████  | 285/351 [04:47<01:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  81%|████████▏ | 286/351 [04:48<01:04,  1.02it/s]\u001b[A\n",
            "\rIteration:  82%|████████▏ | 287/351 [04:49<01:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  82%|████████▏ | 288/351 [04:50<01:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  82%|████████▏ | 289/351 [04:51<01:01,  1.01it/s]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 290/351 [04:52<01:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 291/351 [04:53<00:59,  1.01it/s]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 292/351 [04:54<00:58,  1.01it/s]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 293/351 [04:55<00:57,  1.01it/s]\u001b[A\n",
            "\rIteration:  84%|████████▍ | 294/351 [04:56<00:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  84%|████████▍ | 295/351 [04:57<00:55,  1.01it/s]\u001b[A\n",
            "\rIteration:  84%|████████▍ | 296/351 [04:58<00:54,  1.02it/s]\u001b[A\n",
            "\rIteration:  85%|████████▍ | 297/351 [04:59<00:52,  1.02it/s]\u001b[A\n",
            "\rIteration:  85%|████████▍ | 298/351 [05:00<00:51,  1.03it/s]\u001b[A\n",
            "\rIteration:  85%|████████▌ | 299/351 [05:01<00:50,  1.02it/s]\u001b[A\n",
            "\rIteration:  85%|████████▌ | 300/351 [05:02<00:49,  1.02it/s]\u001b[A\n",
            "\rIteration:  86%|████████▌ | 301/351 [05:03<00:49,  1.02it/s]\u001b[A\n",
            "\rIteration:  86%|████████▌ | 302/351 [05:04<00:48,  1.02it/s]\u001b[A\n",
            "\rIteration:  86%|████████▋ | 303/351 [05:05<00:47,  1.02it/s]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 304/351 [05:06<00:46,  1.01it/s]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 305/351 [05:07<00:45,  1.01it/s]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 306/351 [05:08<00:44,  1.01it/s]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 307/351 [05:09<00:43,  1.02it/s]\u001b[A\n",
            "\rIteration:  88%|████████▊ | 308/351 [05:10<00:42,  1.01it/s]\u001b[A\n",
            "\rIteration:  88%|████████▊ | 309/351 [05:11<00:41,  1.01it/s]\u001b[A\n",
            "\rIteration:  88%|████████▊ | 310/351 [05:12<00:40,  1.01it/s]\u001b[A\n",
            "\rIteration:  89%|████████▊ | 311/351 [05:13<00:39,  1.01it/s]\u001b[A\n",
            "\rIteration:  89%|████████▉ | 312/351 [05:14<00:38,  1.01it/s]\u001b[A\n",
            "\rIteration:  89%|████████▉ | 313/351 [05:15<00:37,  1.01it/s]\u001b[A\n",
            "\rIteration:  89%|████████▉ | 314/351 [05:16<00:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  90%|████████▉ | 315/351 [05:17<00:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  90%|█████████ | 316/351 [05:18<00:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  90%|█████████ | 317/351 [05:19<00:33,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████ | 318/351 [05:20<00:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████ | 319/351 [05:21<00:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████ | 320/351 [05:22<00:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████▏| 321/351 [05:23<00:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  92%|█████████▏| 322/351 [05:24<00:28,  1.01it/s]\u001b[A\n",
            "\rIteration:  92%|█████████▏| 323/351 [05:25<00:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  92%|█████████▏| 324/351 [05:26<00:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 325/351 [05:27<00:25,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 326/351 [05:28<00:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 327/351 [05:29<00:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 328/351 [05:30<00:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  94%|█████████▎| 329/351 [05:31<00:21,  1.01it/s]\u001b[A\n",
            "\rIteration:  94%|█████████▍| 330/351 [05:32<00:20,  1.01it/s]\u001b[A\n",
            "\rIteration:  94%|█████████▍| 331/351 [05:33<00:19,  1.01it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▍| 332/351 [05:34<00:18,  1.01it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▍| 333/351 [05:35<00:17,  1.02it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▌| 334/351 [05:36<00:16,  1.01it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▌| 335/351 [05:37<00:15,  1.01it/s]\u001b[A\n",
            "\rIteration:  96%|█████████▌| 336/351 [05:38<00:14,  1.01it/s]\u001b[A\n",
            "\rIteration:  96%|█████████▌| 337/351 [05:39<00:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  96%|█████████▋| 338/351 [05:40<00:12,  1.01it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 339/351 [05:41<00:11,  1.01it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 340/351 [05:42<00:10,  1.02it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 341/351 [05:43<00:09,  1.02it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 342/351 [05:44<00:08,  1.01it/s]\u001b[A\n",
            "\rIteration:  98%|█████████▊| 343/351 [05:45<00:07,  1.01it/s]\u001b[A\n",
            "\rIteration:  98%|█████████▊| 344/351 [05:46<00:06,  1.01it/s]\u001b[A\n",
            "\rIteration:  98%|█████████▊| 345/351 [05:47<00:05,  1.02it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▊| 346/351 [05:48<00:04,  1.01it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▉| 347/351 [05:49<00:03,  1.02it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▉| 348/351 [05:50<00:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▉| 349/351 [05:51<00:01,  1.01it/s]\u001b[A\n",
            "\rIteration: 100%|█████████▉| 350/351 [05:52<00:00,  1.01it/s]\u001b[A\n",
            "\rIteration: 100%|██████████| 351/351 [05:53<00:00,  1.01it/s]\u001b[A\rIteration: 100%|██████████| 351/351 [05:53<00:00,  1.01s/it]\n",
            "\rEpoch:  67%|██████▋   | 2/3 [11:42<05:50, 350.22s/it]\n",
            "\rIteration:   0%|          | 0/351 [00:00<?, ?it/s]\u001b[A\n",
            "\rIteration:   0%|          | 1/351 [00:00<05:46,  1.01it/s]\u001b[A\n",
            "\rIteration:   1%|          | 2/351 [00:01<05:45,  1.01it/s]\u001b[A\n",
            "\rIteration:   1%|          | 3/351 [00:02<05:45,  1.01it/s]\u001b[A\n",
            "\rIteration:   1%|          | 4/351 [00:03<05:43,  1.01it/s]\u001b[A\n",
            "\rIteration:   1%|▏         | 5/351 [00:04<05:42,  1.01it/s]\u001b[A\n",
            "\rIteration:   2%|▏         | 6/351 [00:05<05:41,  1.01it/s]\u001b[A\n",
            "\rIteration:   2%|▏         | 7/351 [00:06<05:39,  1.01it/s]\u001b[A\n",
            "\rIteration:   2%|▏         | 8/351 [00:07<05:37,  1.02it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 9/351 [00:08<05:36,  1.02it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 10/351 [00:09<05:36,  1.01it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 11/351 [00:10<05:35,  1.01it/s]\u001b[A\n",
            "\rIteration:   3%|▎         | 12/351 [00:11<05:34,  1.01it/s]\u001b[A\n",
            "\rIteration:   4%|▎         | 13/351 [00:12<05:33,  1.01it/s]\u001b[A\n",
            "\rIteration:   4%|▍         | 14/351 [00:13<05:31,  1.02it/s]\u001b[A\n",
            "\rIteration:   4%|▍         | 15/351 [00:14<05:30,  1.02it/s]\u001b[A\n",
            "\rIteration:   5%|▍         | 16/351 [00:15<05:30,  1.01it/s]\u001b[A\n",
            "\rIteration:   5%|▍         | 17/351 [00:16<05:28,  1.02it/s]\u001b[A\n",
            "\rIteration:   5%|▌         | 18/351 [00:17<05:28,  1.01it/s]\u001b[A\n",
            "\rIteration:   5%|▌         | 19/351 [00:18<05:29,  1.01it/s]\u001b[A\n",
            "\rIteration:   6%|▌         | 20/351 [00:19<05:28,  1.01it/s]\u001b[A\n",
            "\rIteration:   6%|▌         | 21/351 [00:20<05:25,  1.01it/s]\u001b[A\n",
            "\rIteration:   6%|▋         | 22/351 [00:21<05:25,  1.01it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 23/351 [00:22<05:24,  1.01it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 24/351 [00:23<05:22,  1.01it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 25/351 [00:24<05:19,  1.02it/s]\u001b[A\n",
            "\rIteration:   7%|▋         | 26/351 [00:25<05:17,  1.02it/s]\u001b[A\n",
            "\rIteration:   8%|▊         | 27/351 [00:26<05:19,  1.02it/s]\u001b[A\n",
            "\rIteration:   8%|▊         | 28/351 [00:27<05:20,  1.01it/s]\u001b[A\n",
            "\rIteration:   8%|▊         | 29/351 [00:28<05:17,  1.01it/s]\u001b[A\n",
            "\rIteration:   9%|▊         | 30/351 [00:29<05:15,  1.02it/s]\u001b[A\n",
            "\rIteration:   9%|▉         | 31/351 [00:30<05:14,  1.02it/s]\u001b[A\n",
            "\rIteration:   9%|▉         | 32/351 [00:31<05:14,  1.01it/s]\u001b[A\n",
            "\rIteration:   9%|▉         | 33/351 [00:32<05:14,  1.01it/s]\u001b[A\n",
            "\rIteration:  10%|▉         | 34/351 [00:33<05:14,  1.01it/s]\u001b[A\n",
            "\rIteration:  10%|▉         | 35/351 [00:34<05:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  10%|█         | 36/351 [00:35<05:12,  1.01it/s]\u001b[A\n",
            "\rIteration:  11%|█         | 37/351 [00:36<05:11,  1.01it/s]\u001b[A\n",
            "\rIteration:  11%|█         | 38/351 [00:37<05:10,  1.01it/s]\u001b[A\n",
            "\rIteration:  11%|█         | 39/351 [00:38<05:09,  1.01it/s]\u001b[A\n",
            "\rIteration:  11%|█▏        | 40/351 [00:39<05:08,  1.01it/s]\u001b[A\n",
            "\rIteration:  12%|█▏        | 41/351 [00:40<05:06,  1.01it/s]\u001b[A\n",
            "\rIteration:  12%|█▏        | 42/351 [00:41<05:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  12%|█▏        | 43/351 [00:42<05:04,  1.01it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 44/351 [00:43<05:04,  1.01it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 45/351 [00:44<05:01,  1.01it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 46/351 [00:45<05:01,  1.01it/s]\u001b[A\n",
            "\rIteration:  13%|█▎        | 47/351 [00:46<05:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  14%|█▎        | 48/351 [00:47<05:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  14%|█▍        | 49/351 [00:48<05:00,  1.00it/s]\u001b[A\n",
            "\rIteration:  14%|█▍        | 50/351 [00:49<04:57,  1.01it/s]\u001b[A\n",
            "\rIteration:  15%|█▍        | 51/351 [00:50<04:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  15%|█▍        | 52/351 [00:51<04:53,  1.02it/s]\u001b[A\n",
            "\rIteration:  15%|█▌        | 53/351 [00:52<04:52,  1.02it/s]\u001b[A\n",
            "\rIteration:  15%|█▌        | 54/351 [00:53<04:52,  1.02it/s]\u001b[A\n",
            "\rIteration:  16%|█▌        | 55/351 [00:54<04:51,  1.02it/s]\u001b[A\n",
            "\rIteration:  16%|█▌        | 56/351 [00:55<04:49,  1.02it/s]\u001b[A\n",
            "\rIteration:  16%|█▌        | 57/351 [00:56<04:49,  1.01it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 58/351 [00:57<04:49,  1.01it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 59/351 [00:58<04:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 60/351 [00:59<04:46,  1.02it/s]\u001b[A\n",
            "\rIteration:  17%|█▋        | 61/351 [01:00<04:45,  1.02it/s]\u001b[A\n",
            "\rIteration:  18%|█▊        | 62/351 [01:01<04:44,  1.02it/s]\u001b[A\n",
            "\rIteration:  18%|█▊        | 63/351 [01:02<04:43,  1.02it/s]\u001b[A\n",
            "\rIteration:  18%|█▊        | 64/351 [01:03<04:41,  1.02it/s]\u001b[A\n",
            "\rIteration:  19%|█▊        | 65/351 [01:04<04:40,  1.02it/s]\u001b[A\n",
            "\rIteration:  19%|█▉        | 66/351 [01:05<04:39,  1.02it/s]\u001b[A\n",
            "\rIteration:  19%|█▉        | 67/351 [01:06<04:38,  1.02it/s]\u001b[A\n",
            "\rIteration:  19%|█▉        | 68/351 [01:07<04:38,  1.02it/s]\u001b[A\n",
            "\rIteration:  20%|█▉        | 69/351 [01:08<04:37,  1.02it/s]\u001b[A\n",
            "\rIteration:  20%|█▉        | 70/351 [01:09<04:37,  1.01it/s]\u001b[A\n",
            "\rIteration:  20%|██        | 71/351 [01:10<04:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  21%|██        | 72/351 [01:11<04:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  21%|██        | 73/351 [01:12<04:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  21%|██        | 74/351 [01:13<04:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  21%|██▏       | 75/351 [01:14<04:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  22%|██▏       | 76/351 [01:15<04:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  22%|██▏       | 77/351 [01:16<04:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  22%|██▏       | 78/351 [01:16<04:28,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 79/351 [01:17<04:28,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 80/351 [01:18<04:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 81/351 [01:19<04:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  23%|██▎       | 82/351 [01:20<04:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  24%|██▎       | 83/351 [01:21<04:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  24%|██▍       | 84/351 [01:22<04:22,  1.02it/s]\u001b[A\n",
            "\rIteration:  24%|██▍       | 85/351 [01:23<04:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  25%|██▍       | 86/351 [01:24<04:21,  1.01it/s]\u001b[A\n",
            "\rIteration:  25%|██▍       | 87/351 [01:25<04:20,  1.02it/s]\u001b[A\n",
            "\rIteration:  25%|██▌       | 88/351 [01:26<04:18,  1.02it/s]\u001b[A\n",
            "\rIteration:  25%|██▌       | 89/351 [01:27<04:17,  1.02it/s]\u001b[A\n",
            "\rIteration:  26%|██▌       | 90/351 [01:28<04:17,  1.01it/s]\u001b[A\n",
            "\rIteration:  26%|██▌       | 91/351 [01:29<04:16,  1.01it/s]\u001b[A\n",
            "\rIteration:  26%|██▌       | 92/351 [01:30<04:15,  1.01it/s]\u001b[A\n",
            "\rIteration:  26%|██▋       | 93/351 [01:31<04:14,  1.01it/s]\u001b[A\n",
            "\rIteration:  27%|██▋       | 94/351 [01:32<04:15,  1.01it/s]\u001b[A\n",
            "\rIteration:  27%|██▋       | 95/351 [01:33<04:15,  1.00it/s]\u001b[A\n",
            "\rIteration:  27%|██▋       | 96/351 [01:34<04:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 97/351 [01:35<04:12,  1.01it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 98/351 [01:36<04:12,  1.00it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 99/351 [01:37<04:10,  1.01it/s]\u001b[A\n",
            "\rIteration:  28%|██▊       | 100/351 [01:38<04:08,  1.01it/s]\u001b[A\n",
            "\rIteration:  29%|██▉       | 101/351 [01:39<04:07,  1.01it/s]\u001b[A\n",
            "\rIteration:  29%|██▉       | 102/351 [01:40<04:07,  1.01it/s]\u001b[A\n",
            "\rIteration:  29%|██▉       | 103/351 [01:41<04:04,  1.01it/s]\u001b[A\n",
            "\rIteration:  30%|██▉       | 104/351 [01:42<04:01,  1.02it/s]\u001b[A\n",
            "\rIteration:  30%|██▉       | 105/351 [01:43<03:59,  1.03it/s]\u001b[A\n",
            "\rIteration:  30%|███       | 106/351 [01:44<03:58,  1.03it/s]\u001b[A\n",
            "\rIteration:  30%|███       | 107/351 [01:45<03:58,  1.02it/s]\u001b[A\n",
            "\rIteration:  31%|███       | 108/351 [01:46<03:59,  1.02it/s]\u001b[A\n",
            "\rIteration:  31%|███       | 109/351 [01:47<03:58,  1.02it/s]\u001b[A\n",
            "\rIteration:  31%|███▏      | 110/351 [01:48<03:57,  1.01it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 111/351 [01:49<03:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 112/351 [01:50<03:55,  1.01it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 113/351 [01:51<03:53,  1.02it/s]\u001b[A\n",
            "\rIteration:  32%|███▏      | 114/351 [01:52<03:52,  1.02it/s]\u001b[A\n",
            "\rIteration:  33%|███▎      | 115/351 [01:53<03:51,  1.02it/s]\u001b[A\n",
            "\rIteration:  33%|███▎      | 116/351 [01:54<03:50,  1.02it/s]\u001b[A\n",
            "\rIteration:  33%|███▎      | 117/351 [01:55<03:49,  1.02it/s]\u001b[A\n",
            "\rIteration:  34%|███▎      | 118/351 [01:56<03:48,  1.02it/s]\u001b[A\n",
            "\rIteration:  34%|███▍      | 119/351 [01:57<03:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  34%|███▍      | 120/351 [01:58<03:47,  1.02it/s]\u001b[A\n",
            "\rIteration:  34%|███▍      | 121/351 [01:59<03:45,  1.02it/s]\u001b[A\n",
            "\rIteration:  35%|███▍      | 122/351 [02:00<03:44,  1.02it/s]\u001b[A\n",
            "\rIteration:  35%|███▌      | 123/351 [02:01<03:44,  1.02it/s]\u001b[A\n",
            "\rIteration:  35%|███▌      | 124/351 [02:02<03:44,  1.01it/s]\u001b[A\n",
            "\rIteration:  36%|███▌      | 125/351 [02:03<03:43,  1.01it/s]\u001b[A\n",
            "\rIteration:  36%|███▌      | 126/351 [02:04<03:42,  1.01it/s]\u001b[A\n",
            "\rIteration:  36%|███▌      | 127/351 [02:05<03:41,  1.01it/s]\u001b[A\n",
            "\rIteration:  36%|███▋      | 128/351 [02:06<03:40,  1.01it/s]\u001b[A\n",
            "\rIteration:  37%|███▋      | 129/351 [02:07<03:39,  1.01it/s]\u001b[A\n",
            "\rIteration:  37%|███▋      | 130/351 [02:08<03:38,  1.01it/s]\u001b[A\n",
            "\rIteration:  37%|███▋      | 131/351 [02:09<03:37,  1.01it/s]\u001b[A\n",
            "\rIteration:  38%|███▊      | 132/351 [02:10<03:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  38%|███▊      | 133/351 [02:11<03:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  38%|███▊      | 134/351 [02:12<03:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  38%|███▊      | 135/351 [02:13<03:32,  1.02it/s]\u001b[A\n",
            "\rIteration:  39%|███▊      | 136/351 [02:14<03:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  39%|███▉      | 137/351 [02:15<03:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  39%|███▉      | 138/351 [02:16<03:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  40%|███▉      | 139/351 [02:17<03:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  40%|███▉      | 140/351 [02:18<03:28,  1.01it/s]\u001b[A\n",
            "\rIteration:  40%|████      | 141/351 [02:19<03:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  40%|████      | 142/351 [02:20<03:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  41%|████      | 143/351 [02:21<03:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  41%|████      | 144/351 [02:22<03:23,  1.02it/s]\u001b[A\n",
            "\rIteration:  41%|████▏     | 145/351 [02:23<03:22,  1.02it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 146/351 [02:24<03:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 147/351 [02:25<03:20,  1.02it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 148/351 [02:26<03:19,  1.02it/s]\u001b[A\n",
            "\rIteration:  42%|████▏     | 149/351 [02:27<03:18,  1.02it/s]\u001b[A\n",
            "\rIteration:  43%|████▎     | 150/351 [02:27<03:17,  1.02it/s]\u001b[A\n",
            "\rIteration:  43%|████▎     | 151/351 [02:28<03:16,  1.02it/s]\u001b[A\n",
            "\rIteration:  43%|████▎     | 152/351 [02:29<03:15,  1.02it/s]\u001b[A\n",
            "\rIteration:  44%|████▎     | 153/351 [02:30<03:14,  1.02it/s]\u001b[A\n",
            "\rIteration:  44%|████▍     | 154/351 [02:31<03:13,  1.02it/s]\u001b[A\n",
            "\rIteration:  44%|████▍     | 155/351 [02:32<03:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  44%|████▍     | 156/351 [02:33<03:12,  1.01it/s]\u001b[A\n",
            "\rIteration:  45%|████▍     | 157/351 [02:34<03:11,  1.01it/s]\u001b[A\n",
            "\rIteration:  45%|████▌     | 158/351 [02:35<03:10,  1.01it/s]\u001b[A\n",
            "\rIteration:  45%|████▌     | 159/351 [02:36<03:10,  1.01it/s]\u001b[A\n",
            "\rIteration:  46%|████▌     | 160/351 [02:37<03:09,  1.01it/s]\u001b[A\n",
            "\rIteration:  46%|████▌     | 161/351 [02:38<03:08,  1.01it/s]\u001b[A\n",
            "\rIteration:  46%|████▌     | 162/351 [02:39<03:06,  1.01it/s]\u001b[A\n",
            "\rIteration:  46%|████▋     | 163/351 [02:40<03:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  47%|████▋     | 164/351 [02:41<03:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  47%|████▋     | 165/351 [02:42<03:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  47%|████▋     | 166/351 [02:43<03:02,  1.02it/s]\u001b[A\n",
            "\rIteration:  48%|████▊     | 167/351 [02:44<03:01,  1.01it/s]\u001b[A\n",
            "\rIteration:  48%|████▊     | 168/351 [02:45<03:00,  1.02it/s]\u001b[A\n",
            "\rIteration:  48%|████▊     | 169/351 [02:46<02:58,  1.02it/s]\u001b[A\n",
            "\rIteration:  48%|████▊     | 170/351 [02:47<02:58,  1.02it/s]\u001b[A\n",
            "\rIteration:  49%|████▊     | 171/351 [02:48<02:57,  1.02it/s]\u001b[A\n",
            "\rIteration:  49%|████▉     | 172/351 [02:49<02:56,  1.02it/s]\u001b[A\n",
            "\rIteration:  49%|████▉     | 173/351 [02:50<02:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  50%|████▉     | 174/351 [02:51<02:56,  1.00it/s]\u001b[A\n",
            "\rIteration:  50%|████▉     | 175/351 [02:52<02:54,  1.01it/s]\u001b[A\n",
            "\rIteration:  50%|█████     | 176/351 [02:53<02:53,  1.01it/s]\u001b[A\n",
            "\rIteration:  50%|█████     | 177/351 [02:54<02:51,  1.01it/s]\u001b[A\n",
            "\rIteration:  51%|█████     | 178/351 [02:55<02:51,  1.01it/s]\u001b[A\n",
            "\rIteration:  51%|█████     | 179/351 [02:56<02:49,  1.01it/s]\u001b[A\n",
            "\rIteration:  51%|█████▏    | 180/351 [02:57<02:49,  1.01it/s]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 181/351 [02:58<02:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 182/351 [02:59<02:46,  1.02it/s]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 183/351 [03:00<02:44,  1.02it/s]\u001b[A\n",
            "\rIteration:  52%|█████▏    | 184/351 [03:01<02:42,  1.03it/s]\u001b[A\n",
            "\rIteration:  53%|█████▎    | 185/351 [03:02<02:41,  1.03it/s]\u001b[A\n",
            "\rIteration:  53%|█████▎    | 186/351 [03:03<02:40,  1.03it/s]\u001b[A\n",
            "\rIteration:  53%|█████▎    | 187/351 [03:04<02:40,  1.02it/s]\u001b[A\n",
            "\rIteration:  54%|█████▎    | 188/351 [03:05<02:39,  1.02it/s]\u001b[A\n",
            "\rIteration:  54%|█████▍    | 189/351 [03:06<02:38,  1.02it/s]\u001b[A\n",
            "\rIteration:  54%|█████▍    | 190/351 [03:07<02:39,  1.01it/s]\u001b[A\n",
            "\rIteration:  54%|█████▍    | 191/351 [03:08<02:38,  1.01it/s]\u001b[A\n",
            "\rIteration:  55%|█████▍    | 192/351 [03:09<02:37,  1.01it/s]\u001b[A\n",
            "\rIteration:  55%|█████▍    | 193/351 [03:10<02:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  55%|█████▌    | 194/351 [03:11<02:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  56%|█████▌    | 195/351 [03:12<02:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  56%|█████▌    | 196/351 [03:13<02:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  56%|█████▌    | 197/351 [03:14<02:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  56%|█████▋    | 198/351 [03:15<02:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  57%|█████▋    | 199/351 [03:16<02:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  57%|█████▋    | 200/351 [03:17<02:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  57%|█████▋    | 201/351 [03:18<02:28,  1.01it/s]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 202/351 [03:19<02:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 203/351 [03:20<02:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 204/351 [03:21<02:25,  1.01it/s]\u001b[A\n",
            "\rIteration:  58%|█████▊    | 205/351 [03:22<02:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  59%|█████▊    | 206/351 [03:23<02:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  59%|█████▉    | 207/351 [03:24<02:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  59%|█████▉    | 208/351 [03:25<02:21,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|█████▉    | 209/351 [03:26<02:20,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|█████▉    | 210/351 [03:27<02:20,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|██████    | 211/351 [03:28<02:18,  1.01it/s]\u001b[A\n",
            "\rIteration:  60%|██████    | 212/351 [03:29<02:19,  1.01s/it]\u001b[A\n",
            "\rIteration:  61%|██████    | 213/351 [03:30<02:19,  1.01s/it]\u001b[A\n",
            "\rIteration:  61%|██████    | 214/351 [03:31<02:19,  1.02s/it]\u001b[A\n",
            "\rIteration:  61%|██████▏   | 215/351 [03:32<02:16,  1.00s/it]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 216/351 [03:33<02:17,  1.02s/it]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 217/351 [03:34<02:15,  1.01s/it]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 218/351 [03:35<02:14,  1.01s/it]\u001b[A\n",
            "\rIteration:  62%|██████▏   | 219/351 [03:36<02:12,  1.01s/it]\u001b[A\n",
            "\rIteration:  63%|██████▎   | 220/351 [03:37<02:12,  1.01s/it]\u001b[A\n",
            "\rIteration:  63%|██████▎   | 221/351 [03:38<02:12,  1.02s/it]\u001b[A\n",
            "\rIteration:  63%|██████▎   | 222/351 [03:39<02:10,  1.01s/it]\u001b[A\n",
            "\rIteration:  64%|██████▎   | 223/351 [03:40<02:08,  1.00s/it]\u001b[A\n",
            "\rIteration:  64%|██████▍   | 224/351 [03:41<02:06,  1.00it/s]\u001b[A\n",
            "\rIteration:  64%|██████▍   | 225/351 [03:42<02:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  64%|██████▍   | 226/351 [03:43<02:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  65%|██████▍   | 227/351 [03:44<02:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  65%|██████▍   | 228/351 [03:45<02:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  65%|██████▌   | 229/351 [03:46<02:01,  1.01it/s]\u001b[A\n",
            "\rIteration:  66%|██████▌   | 230/351 [03:47<01:59,  1.01it/s]\u001b[A\n",
            "\rIteration:  66%|██████▌   | 231/351 [03:48<01:58,  1.01it/s]\u001b[A\n",
            "\rIteration:  66%|██████▌   | 232/351 [03:49<01:57,  1.01it/s]\u001b[A\n",
            "\rIteration:  66%|██████▋   | 233/351 [03:50<01:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  67%|██████▋   | 234/351 [03:51<01:55,  1.01it/s]\u001b[A\n",
            "\rIteration:  67%|██████▋   | 235/351 [03:52<01:54,  1.01it/s]\u001b[A\n",
            "\rIteration:  67%|██████▋   | 236/351 [03:53<01:53,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 237/351 [03:54<01:52,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 238/351 [03:55<01:51,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 239/351 [03:56<01:50,  1.01it/s]\u001b[A\n",
            "\rIteration:  68%|██████▊   | 240/351 [03:57<01:49,  1.01it/s]\u001b[A\n",
            "\rIteration:  69%|██████▊   | 241/351 [03:58<01:48,  1.01it/s]\u001b[A\n",
            "\rIteration:  69%|██████▉   | 242/351 [03:59<01:47,  1.02it/s]\u001b[A\n",
            "\rIteration:  69%|██████▉   | 243/351 [04:00<01:46,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|██████▉   | 244/351 [04:01<01:45,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|██████▉   | 245/351 [04:02<01:44,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|███████   | 246/351 [04:03<01:43,  1.01it/s]\u001b[A\n",
            "\rIteration:  70%|███████   | 247/351 [04:04<01:42,  1.01it/s]\u001b[A\n",
            "\rIteration:  71%|███████   | 248/351 [04:05<01:41,  1.02it/s]\u001b[A\n",
            "\rIteration:  71%|███████   | 249/351 [04:06<01:40,  1.02it/s]\u001b[A\n",
            "\rIteration:  71%|███████   | 250/351 [04:07<01:39,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 251/351 [04:08<01:38,  1.02it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 252/351 [04:09<01:38,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 253/351 [04:10<01:37,  1.01it/s]\u001b[A\n",
            "\rIteration:  72%|███████▏  | 254/351 [04:11<01:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  73%|███████▎  | 255/351 [04:11<01:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  73%|███████▎  | 256/351 [04:12<01:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  73%|███████▎  | 257/351 [04:13<01:33,  1.01it/s]\u001b[A\n",
            "\rIteration:  74%|███████▎  | 258/351 [04:14<01:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  74%|███████▍  | 259/351 [04:15<01:31,  1.00it/s]\u001b[A\n",
            "\rIteration:  74%|███████▍  | 260/351 [04:16<01:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  74%|███████▍  | 261/351 [04:17<01:28,  1.01it/s]\u001b[A\n",
            "\rIteration:  75%|███████▍  | 262/351 [04:18<01:27,  1.02it/s]\u001b[A\n",
            "\rIteration:  75%|███████▍  | 263/351 [04:19<01:25,  1.03it/s]\u001b[A\n",
            "\rIteration:  75%|███████▌  | 264/351 [04:20<01:24,  1.03it/s]\u001b[A\n",
            "\rIteration:  75%|███████▌  | 265/351 [04:21<01:23,  1.03it/s]\u001b[A\n",
            "\rIteration:  76%|███████▌  | 266/351 [04:22<01:22,  1.03it/s]\u001b[A\n",
            "\rIteration:  76%|███████▌  | 267/351 [04:23<01:21,  1.03it/s]\u001b[A\n",
            "\rIteration:  76%|███████▋  | 268/351 [04:24<01:20,  1.03it/s]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 269/351 [04:25<01:19,  1.03it/s]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 270/351 [04:26<01:19,  1.02it/s]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 271/351 [04:27<01:18,  1.02it/s]\u001b[A\n",
            "\rIteration:  77%|███████▋  | 272/351 [04:28<01:17,  1.02it/s]\u001b[A\n",
            "\rIteration:  78%|███████▊  | 273/351 [04:29<01:16,  1.02it/s]\u001b[A\n",
            "\rIteration:  78%|███████▊  | 274/351 [04:30<01:15,  1.02it/s]\u001b[A\n",
            "\rIteration:  78%|███████▊  | 275/351 [04:31<01:14,  1.02it/s]\u001b[A\n",
            "\rIteration:  79%|███████▊  | 276/351 [04:32<01:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  79%|███████▉  | 277/351 [04:33<01:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  79%|███████▉  | 278/351 [04:34<01:11,  1.02it/s]\u001b[A\n",
            "\rIteration:  79%|███████▉  | 279/351 [04:35<01:11,  1.01it/s]\u001b[A\n",
            "\rIteration:  80%|███████▉  | 280/351 [04:36<01:10,  1.01it/s]\u001b[A\n",
            "\rIteration:  80%|████████  | 281/351 [04:37<01:09,  1.01it/s]\u001b[A\n",
            "\rIteration:  80%|████████  | 282/351 [04:38<01:08,  1.01it/s]\u001b[A\n",
            "\rIteration:  81%|████████  | 283/351 [04:39<01:07,  1.01it/s]\u001b[A\n",
            "\rIteration:  81%|████████  | 284/351 [04:40<01:06,  1.01it/s]\u001b[A\n",
            "\rIteration:  81%|████████  | 285/351 [04:41<01:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  81%|████████▏ | 286/351 [04:42<01:04,  1.01it/s]\u001b[A\n",
            "\rIteration:  82%|████████▏ | 287/351 [04:43<01:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  82%|████████▏ | 288/351 [04:44<01:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  82%|████████▏ | 289/351 [04:45<01:01,  1.00it/s]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 290/351 [04:46<01:00,  1.01it/s]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 291/351 [04:47<00:59,  1.01it/s]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 292/351 [04:48<00:58,  1.01it/s]\u001b[A\n",
            "\rIteration:  83%|████████▎ | 293/351 [04:49<00:57,  1.01it/s]\u001b[A\n",
            "\rIteration:  84%|████████▍ | 294/351 [04:50<00:56,  1.01it/s]\u001b[A\n",
            "\rIteration:  84%|████████▍ | 295/351 [04:51<00:55,  1.01it/s]\u001b[A\n",
            "\rIteration:  84%|████████▍ | 296/351 [04:52<00:54,  1.01it/s]\u001b[A\n",
            "\rIteration:  85%|████████▍ | 297/351 [04:53<00:53,  1.01it/s]\u001b[A\r                                                     \r\n",
            "\r                                                            \r\u001b[A\rEpoch:  67%|██████▋   | 2/3 [16:36<05:50, 350.22s/it]\n",
            "\rIteration:  85%|████████▍ | 297/351 [04:54<00:53,  1.01it/s]\u001b[A04/16/2022 10:40:39 - INFO - transformers.trainer -   Saving model checkpoint to pred/checkpoint-1000\n",
            "04/16/2022 10:40:39 - INFO - transformers.configuration_utils -   Configuration saved in pred/checkpoint-1000/config.json\n",
            "04/16/2022 10:40:41 - INFO - transformers.modeling_utils -   Model weights saved in pred/checkpoint-1000/pytorch_model.bin\n",
            "04/16/2022 10:40:44 - INFO - transformers.trainer -   Saving optimizer and scheduler states to pred/checkpoint-1000\n",
            "\n",
            "\rIteration:  85%|████████▍ | 298/351 [04:59<02:16,  2.57s/it]\u001b[A\n",
            "\rIteration:  85%|████████▌ | 299/351 [05:00<01:50,  2.13s/it]\u001b[A\n",
            "\rIteration:  85%|████████▌ | 300/351 [05:01<01:31,  1.79s/it]\u001b[A\n",
            "\rIteration:  86%|████████▌ | 301/351 [05:02<01:17,  1.55s/it]\u001b[A\n",
            "\rIteration:  86%|████████▌ | 302/351 [05:03<01:07,  1.38s/it]\u001b[A\n",
            "\rIteration:  86%|████████▋ | 303/351 [05:04<01:00,  1.26s/it]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 304/351 [05:05<00:55,  1.18s/it]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 305/351 [05:06<00:51,  1.12s/it]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 306/351 [05:07<00:48,  1.08s/it]\u001b[A\n",
            "\rIteration:  87%|████████▋ | 307/351 [05:08<00:46,  1.05s/it]\u001b[A\n",
            "\rIteration:  88%|████████▊ | 308/351 [05:09<00:44,  1.03s/it]\u001b[A\n",
            "\rIteration:  88%|████████▊ | 309/351 [05:10<00:42,  1.02s/it]\u001b[A\n",
            "\rIteration:  88%|████████▊ | 310/351 [05:11<00:41,  1.01s/it]\u001b[A\n",
            "\rIteration:  89%|████████▊ | 311/351 [05:12<00:40,  1.01s/it]\u001b[A\n",
            "\rIteration:  89%|████████▉ | 312/351 [05:13<00:38,  1.00it/s]\u001b[A\n",
            "\rIteration:  89%|████████▉ | 313/351 [05:14<00:37,  1.00it/s]\u001b[A\n",
            "\rIteration:  89%|████████▉ | 314/351 [05:15<00:36,  1.01it/s]\u001b[A\n",
            "\rIteration:  90%|████████▉ | 315/351 [05:16<00:35,  1.01it/s]\u001b[A\n",
            "\rIteration:  90%|█████████ | 316/351 [05:17<00:34,  1.01it/s]\u001b[A\n",
            "\rIteration:  90%|█████████ | 317/351 [05:18<00:33,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████ | 318/351 [05:19<00:32,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████ | 319/351 [05:20<00:31,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████ | 320/351 [05:21<00:30,  1.01it/s]\u001b[A\n",
            "\rIteration:  91%|█████████▏| 321/351 [05:22<00:29,  1.01it/s]\u001b[A\n",
            "\rIteration:  92%|█████████▏| 322/351 [05:23<00:28,  1.02it/s]\u001b[A\n",
            "\rIteration:  92%|█████████▏| 323/351 [05:24<00:27,  1.01it/s]\u001b[A\n",
            "\rIteration:  92%|█████████▏| 324/351 [05:25<00:26,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 325/351 [05:26<00:25,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 326/351 [05:27<00:24,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 327/351 [05:28<00:23,  1.01it/s]\u001b[A\n",
            "\rIteration:  93%|█████████▎| 328/351 [05:29<00:22,  1.01it/s]\u001b[A\n",
            "\rIteration:  94%|█████████▎| 329/351 [05:30<00:21,  1.01it/s]\u001b[A\n",
            "\rIteration:  94%|█████████▍| 330/351 [05:31<00:20,  1.01it/s]\u001b[A\n",
            "\rIteration:  94%|█████████▍| 331/351 [05:32<00:19,  1.01it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▍| 332/351 [05:33<00:18,  1.01it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▍| 333/351 [05:34<00:17,  1.01it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▌| 334/351 [05:35<00:16,  1.01it/s]\u001b[A\n",
            "\rIteration:  95%|█████████▌| 335/351 [05:36<00:15,  1.01it/s]\u001b[A\n",
            "\rIteration:  96%|█████████▌| 336/351 [05:37<00:14,  1.00it/s]\u001b[A\n",
            "\rIteration:  96%|█████████▌| 337/351 [05:38<00:13,  1.01it/s]\u001b[A\n",
            "\rIteration:  96%|█████████▋| 338/351 [05:39<00:12,  1.02it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 339/351 [05:40<00:11,  1.02it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 340/351 [05:41<00:10,  1.03it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 341/351 [05:42<00:09,  1.02it/s]\u001b[A\n",
            "\rIteration:  97%|█████████▋| 342/351 [05:43<00:08,  1.02it/s]\u001b[A\n",
            "\rIteration:  98%|█████████▊| 343/351 [05:44<00:07,  1.02it/s]\u001b[A\n",
            "\rIteration:  98%|█████████▊| 344/351 [05:45<00:06,  1.02it/s]\u001b[A\n",
            "\rIteration:  98%|█████████▊| 345/351 [05:46<00:05,  1.01it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▊| 346/351 [05:47<00:04,  1.01it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▉| 347/351 [05:48<00:03,  1.01it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▉| 348/351 [05:49<00:02,  1.01it/s]\u001b[A\n",
            "\rIteration:  99%|█████████▉| 349/351 [05:50<00:01,  1.01it/s]\u001b[A\n",
            "\rIteration: 100%|█████████▉| 350/351 [05:51<00:00,  1.01it/s]\u001b[A\n",
            "\rIteration: 100%|██████████| 351/351 [05:52<00:00,  1.02it/s]\u001b[A\rIteration: 100%|██████████| 351/351 [05:52<00:00,  1.00s/it]\n",
            "\rEpoch: 100%|██████████| 3/3 [17:34<00:00, 350.78s/it]\rEpoch: 100%|██████████| 3/3 [17:34<00:00, 351.38s/it]\n",
            "04/16/2022 10:41:37 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "04/16/2022 10:41:37 - INFO - transformers.trainer -   Saving model checkpoint to pred\n",
            "04/16/2022 10:41:37 - INFO - transformers.configuration_utils -   Configuration saved in pred/config.json\n",
            "04/16/2022 10:41:39 - INFO - transformers.modeling_utils -   Model weights saved in pred/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"bert-base-uncased\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=pred\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_train \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvVgwitC9zgo",
        "outputId": "5934cc2c-3836-430d-fc07-a737de7860ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/FinCausal-2020/pred/config.json (deflated 61%)\n",
            "  adding: content/FinCausal-2020/pred/pytorch_model.bin (deflated 7%)\n",
            "  adding: content/FinCausal-2020/pred/special_tokens_map.json (deflated 40%)\n",
            "  adding: content/FinCausal-2020/pred/tokenizer_config.json (stored 0%)\n",
            "  adding: content/FinCausal-2020/pred/training_args.bin (deflated 43%)\n",
            "  adding: content/FinCausal-2020/pred/vocab.txt (deflated 53%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/FinCausal-2020/model_bert_till_2022_train.zip /content/FinCausal-2020/pred/*.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LIUWu535-dQd",
        "outputId": "7534cc41-439e-41d7-b78e-3384f5cc5183"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_8ed0f644-264d-4dc5-9a5f-f3a5d80c171a\", \"model_bert_till_2022_train.zip\", 405380731)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download(\"/content/FinCausal-2020/model_bert_till_2022_train.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wStjhvs-7Fsb"
      },
      "source": [
        "## Predictions BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkjcF5Fm_dZt"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/FinCausal-2020/pred_new_model/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX1Nzarr5vLS",
        "outputId": "05e680b5-5bf5-4d45-bdf8-1d37d71f5c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_labels: ['_', 'B-C', 'I-C', 'B-E', 'I-E']\n",
            "pos_labels: ['<pad>', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'GW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
            "Number of pos_labels: 50\n",
            "ADD_POS: False\n",
            "pos_labels: ['<pad>', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'GW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
            "Number of pos_labels: 50\n",
            "Number of exact matches: 2 out of 933\n",
            "933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "04/16/2022 10:50:26 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "04/16/2022 10:50:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/16/2022 10:50:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='pred_new_model', overwrite_output_dir=False, add_pos=False, do_train=False, do_eval=False, do_predict=True, evaluate_during_training=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir=None, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False)\n",
            "04/16/2022 10:50:26 - INFO - transformers.configuration_utils -   loading configuration file /content/FinCausal-2020/pred/config.json\n",
            "04/16/2022 10:50:26 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"_\",\n",
            "    \"1\": \"B-C\",\n",
            "    \"2\": \"I-C\",\n",
            "    \"3\": \"B-E\",\n",
            "    \"4\": \"I-E\"\n",
            "  },\n",
            "  \"id2poslabel\": {\n",
            "    \"0\": \"<pad>\",\n",
            "    \"1\": \"$\",\n",
            "    \"2\": \"''\",\n",
            "    \"3\": \",\",\n",
            "    \"4\": \"-LRB-\",\n",
            "    \"5\": \"-RRB-\",\n",
            "    \"6\": \".\",\n",
            "    \"7\": \":\",\n",
            "    \"8\": \"ADD\",\n",
            "    \"9\": \"AFX\",\n",
            "    \"10\": \"CC\",\n",
            "    \"11\": \"CD\",\n",
            "    \"12\": \"DT\",\n",
            "    \"13\": \"EX\",\n",
            "    \"14\": \"FW\",\n",
            "    \"15\": \"GW\",\n",
            "    \"16\": \"HYPH\",\n",
            "    \"17\": \"IN\",\n",
            "    \"18\": \"JJ\",\n",
            "    \"19\": \"JJR\",\n",
            "    \"20\": \"JJS\",\n",
            "    \"21\": \"LS\",\n",
            "    \"22\": \"MD\",\n",
            "    \"23\": \"NFP\",\n",
            "    \"24\": \"NN\",\n",
            "    \"25\": \"NNP\",\n",
            "    \"26\": \"NNPS\",\n",
            "    \"27\": \"NNS\",\n",
            "    \"28\": \"PDT\",\n",
            "    \"29\": \"POS\",\n",
            "    \"30\": \"PRP\",\n",
            "    \"31\": \"PRP$\",\n",
            "    \"32\": \"RB\",\n",
            "    \"33\": \"RBR\",\n",
            "    \"34\": \"RBS\",\n",
            "    \"35\": \"RP\",\n",
            "    \"36\": \"SYM\",\n",
            "    \"37\": \"TO\",\n",
            "    \"38\": \"UH\",\n",
            "    \"39\": \"VB\",\n",
            "    \"40\": \"VBD\",\n",
            "    \"41\": \"VBG\",\n",
            "    \"42\": \"VBN\",\n",
            "    \"43\": \"VBP\",\n",
            "    \"44\": \"VBZ\",\n",
            "    \"45\": \"WDT\",\n",
            "    \"46\": \"WP\",\n",
            "    \"47\": \"WP$\",\n",
            "    \"48\": \"WRB\",\n",
            "    \"49\": \"``\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-C\": 1,\n",
            "    \"B-E\": 3,\n",
            "    \"I-C\": 2,\n",
            "    \"I-E\": 4,\n",
            "    \"_\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_pos_labels\": 50,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/16/2022 10:50:26 - INFO - transformers.configuration_utils -   loading configuration file /content/FinCausal-2020/pred/config.json\n",
            "04/16/2022 10:50:26 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"_\",\n",
            "    \"1\": \"B-C\",\n",
            "    \"2\": \"I-C\",\n",
            "    \"3\": \"B-E\",\n",
            "    \"4\": \"I-E\"\n",
            "  },\n",
            "  \"id2poslabel\": {\n",
            "    \"0\": \"<pad>\",\n",
            "    \"1\": \"$\",\n",
            "    \"2\": \"''\",\n",
            "    \"3\": \",\",\n",
            "    \"4\": \"-LRB-\",\n",
            "    \"5\": \"-RRB-\",\n",
            "    \"6\": \".\",\n",
            "    \"7\": \":\",\n",
            "    \"8\": \"ADD\",\n",
            "    \"9\": \"AFX\",\n",
            "    \"10\": \"CC\",\n",
            "    \"11\": \"CD\",\n",
            "    \"12\": \"DT\",\n",
            "    \"13\": \"EX\",\n",
            "    \"14\": \"FW\",\n",
            "    \"15\": \"GW\",\n",
            "    \"16\": \"HYPH\",\n",
            "    \"17\": \"IN\",\n",
            "    \"18\": \"JJ\",\n",
            "    \"19\": \"JJR\",\n",
            "    \"20\": \"JJS\",\n",
            "    \"21\": \"LS\",\n",
            "    \"22\": \"MD\",\n",
            "    \"23\": \"NFP\",\n",
            "    \"24\": \"NN\",\n",
            "    \"25\": \"NNP\",\n",
            "    \"26\": \"NNPS\",\n",
            "    \"27\": \"NNS\",\n",
            "    \"28\": \"PDT\",\n",
            "    \"29\": \"POS\",\n",
            "    \"30\": \"PRP\",\n",
            "    \"31\": \"PRP$\",\n",
            "    \"32\": \"RB\",\n",
            "    \"33\": \"RBR\",\n",
            "    \"34\": \"RBS\",\n",
            "    \"35\": \"RP\",\n",
            "    \"36\": \"SYM\",\n",
            "    \"37\": \"TO\",\n",
            "    \"38\": \"UH\",\n",
            "    \"39\": \"VB\",\n",
            "    \"40\": \"VBD\",\n",
            "    \"41\": \"VBG\",\n",
            "    \"42\": \"VBN\",\n",
            "    \"43\": \"VBP\",\n",
            "    \"44\": \"VBZ\",\n",
            "    \"45\": \"WDT\",\n",
            "    \"46\": \"WP\",\n",
            "    \"47\": \"WP$\",\n",
            "    \"48\": \"WRB\",\n",
            "    \"49\": \"``\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-C\": 1,\n",
            "    \"B-E\": 3,\n",
            "    \"I-C\": 2,\n",
            "    \"I-E\": 4,\n",
            "    \"_\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_pos_labels\": 50,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/16/2022 10:50:26 - INFO - transformers.tokenization_utils -   Model name '/content/FinCausal-2020/pred/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/content/FinCausal-2020/pred/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/16/2022 10:50:26 - INFO - transformers.tokenization_utils -   Didn't find file /content/FinCausal-2020/pred/added_tokens.json. We won't load it.\n",
            "04/16/2022 10:50:26 - INFO - transformers.tokenization_utils -   loading file /content/FinCausal-2020/pred/vocab.txt\n",
            "04/16/2022 10:50:26 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/16/2022 10:50:26 - INFO - transformers.tokenization_utils -   loading file /content/FinCausal-2020/pred/special_tokens_map.json\n",
            "04/16/2022 10:50:26 - INFO - transformers.tokenization_utils -   loading file /content/FinCausal-2020/pred/tokenizer_config.json\n",
            "04/16/2022 10:50:26 - INFO - transformers.modeling_utils -   loading weights file /content/FinCausal-2020/pred/pytorch_model.bin\n",
            "04/16/2022 10:50:31 - INFO - transformers.trainer -   You are instantiating a Trainer but wandb is not installed. Install it to use Weights & Biases logging.\n",
            "04/16/2022 10:50:31 - INFO - utils -   Creating features from dataset file at data\n",
            "04/16/2022 10:50:31 - INFO - utils -   Writing example 0 of 933\n",
            "04/16/2022 10:50:31 - INFO - utils -   *** Example ***\n",
            "04/16/2022 10:50:31 - INFO - utils -   guid: test-1\n",
            "04/16/2022 10:50:31 - INFO - utils -   tokens: [CLS] the israel tax authority claims that when a company controlled by the bros ##h brothers sold a 14 % stake in os ##hra ##d natural gas for 8 . 6 million she ##kel ##s in 2017 , they should have paid 4 . 3 million she ##kel ##s in tax . [SEP]\n",
            "04/16/2022 10:50:31 - INFO - utils -   input_ids: 101 1996 3956 4171 3691 4447 2008 2043 1037 2194 4758 2011 1996 10243 2232 3428 2853 1037 2403 1003 8406 1999 9808 13492 2094 3019 3806 2005 1022 1012 1020 2454 2016 11705 2015 1999 2418 1010 2027 2323 2031 3825 1018 1012 1017 2454 2016 11705 2015 1999 4171 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   label_ids: -100 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 0 0 0 0 0 0 0 0 -100 -100 0 0 0 0 -100 -100 0 0 -100 -100 0 0 0 0 0 0 0 0 -100 -100 0 0 -100 -100 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/16/2022 10:50:31 - INFO - utils -   pos_tag_ids: 0 12 25 25 25 44 17 48 12 24 42 17 12 25 0 27 40 12 11 24 24 17 25 0 0 25 25 17 11 0 0 11 27 0 0 17 11 3 30 22 39 42 11 0 0 11 27 0 0 17 24 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   *** Example ***\n",
            "04/16/2022 10:50:31 - INFO - utils -   guid: test-2\n",
            "04/16/2022 10:50:31 - INFO - utils -   tokens: [CLS] because os ##hra ##d is a privately held business , it was liable for a 26 . 3 % company tax on the sale and the bros ##hes themselves were liable for another 32 % in personal taxes - a combined 50 % in taxes after adjustments . [SEP]\n",
            "04/16/2022 10:50:31 - INFO - utils -   input_ids: 101 2138 9808 13492 2094 2003 1037 9139 2218 2449 1010 2009 2001 20090 2005 1037 2656 1012 1017 1003 2194 4171 2006 1996 5096 1998 1996 10243 15689 3209 2020 20090 2005 2178 3590 1003 1999 3167 7773 1011 1037 4117 2753 1003 1999 7773 2044 24081 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   label_ids: -100 0 0 -100 -100 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 0 0 0 0 0 0 0 0 0 -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/16/2022 10:50:31 - INFO - utils -   pos_tag_ids: 0 17 25 0 0 44 12 32 42 24 3 30 40 18 17 12 11 0 0 24 24 24 17 12 24 10 12 26 0 30 40 18 17 12 11 24 17 18 27 3 12 42 11 24 17 27 17 27 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   *** Example ***\n",
            "04/16/2022 10:50:31 - INFO - utils -   guid: test-3\n",
            "04/16/2022 10:50:31 - INFO - utils -   tokens: [CLS] instead , the company reported itself as a family business , which is liable to a single 32 % tax . [SEP]\n",
            "04/16/2022 10:50:31 - INFO - utils -   input_ids: 101 2612 1010 1996 2194 2988 2993 2004 1037 2155 2449 1010 2029 2003 20090 2000 1037 2309 3590 1003 4171 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   label_ids: -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/16/2022 10:50:31 - INFO - utils -   pos_tag_ids: 0 32 3 12 24 40 30 17 12 24 24 3 45 44 18 17 12 18 11 24 24 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   *** Example ***\n",
            "04/16/2022 10:50:31 - INFO - utils -   guid: test-4\n",
            "04/16/2022 10:50:31 - INFO - utils -   tokens: [CLS] on tuesday , rep ##o loan rates more than triple ##d which threatened the supply of funds to the banking system , and forced the new york federal reserve to flood the market with us $ 75 - billion in loans yesterday and another $ 75 - billion today . [SEP]\n",
            "04/16/2022 10:50:31 - INFO - utils -   input_ids: 101 2006 9857 1010 16360 2080 5414 6165 2062 2084 6420 2094 2029 5561 1996 4425 1997 5029 2000 1996 8169 2291 1010 1998 3140 1996 2047 2259 2976 3914 2000 7186 1996 3006 2007 2149 1002 4293 1011 4551 1999 10940 7483 1998 2178 1002 4293 1011 4551 2651 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   label_ids: -100 0 0 0 0 -100 0 0 0 0 0 -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/16/2022 10:50:31 - INFO - utils -   pos_tag_ids: 0 17 25 3 24 0 24 27 19 17 42 0 45 40 12 24 17 27 17 12 24 24 3 10 40 12 25 25 25 25 37 39 12 24 17 25 1 11 36 11 17 27 24 10 12 1 11 36 11 24 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   *** Example ***\n",
            "04/16/2022 10:50:31 - INFO - utils -   guid: test-5\n",
            "04/16/2022 10:50:31 - INFO - utils -   tokens: [CLS] approximately 144 jurisdictions use if ##rs , as it ' s required for public firms based in south korea , singapore , and in most european countries . [SEP]\n",
            "04/16/2022 10:50:31 - INFO - utils -   input_ids: 101 3155 14748 17370 2224 2065 2869 1010 2004 2009 1005 1055 3223 2005 2270 9786 2241 1999 2148 4420 1010 5264 1010 1998 1999 2087 2647 3032 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:31 - INFO - utils -   label_ids: -100 0 0 0 0 0 -100 0 0 0 0 -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/16/2022 10:50:31 - INFO - utils -   pos_tag_ids: 0 32 11 27 43 24 0 3 17 30 44 0 42 17 18 27 42 17 25 25 3 25 3 10 17 20 18 27 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/16/2022 10:50:34 - INFO - utils -   Saving features into cached file data/cached_test_BertTokenizer_350\n",
            "04/16/2022 10:50:36 - INFO - transformers.trainer -   ***** Running Prediction *****\n",
            "04/16/2022 10:50:36 - INFO - transformers.trainer -     Num examples = 933\n",
            "04/16/2022 10:50:36 - INFO - transformers.trainer -     Batch size = 8\n",
            "\rPrediction:   0%|          | 0/117 [00:00<?, ?it/s]/content/FinCausal-2020/transformers/modeling_bert.py:1482: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  lengths_ = [torch.nonzero(active_loss[i])[-1].item()+1 for i in range(active_loss.size(0))]\n",
            "\rPrediction:   1%|          | 1/117 [00:00<01:44,  1.11it/s]\rPrediction:   2%|▏         | 2/117 [00:01<01:44,  1.10it/s]\rPrediction:   3%|▎         | 3/117 [00:02<01:47,  1.06it/s]\rPrediction:   3%|▎         | 4/117 [00:03<01:46,  1.07it/s]\rPrediction:   4%|▍         | 5/117 [00:05<01:55,  1.03s/it]\rPrediction:   5%|▌         | 6/117 [00:06<01:54,  1.03s/it]\rPrediction:   6%|▌         | 7/117 [00:07<01:55,  1.05s/it]\rPrediction:   7%|▋         | 8/117 [00:08<01:54,  1.05s/it]\rPrediction:   8%|▊         | 9/117 [00:09<01:56,  1.08s/it]\rPrediction:   9%|▊         | 10/117 [00:10<01:51,  1.04s/it]\rPrediction:   9%|▉         | 11/117 [00:11<01:50,  1.04s/it]\rPrediction:  10%|█         | 12/117 [00:12<01:47,  1.02s/it]\rPrediction:  11%|█         | 13/117 [00:13<01:55,  1.11s/it]\rPrediction:  12%|█▏        | 14/117 [00:14<01:46,  1.04s/it]\rPrediction:  13%|█▎        | 15/117 [00:15<01:39,  1.02it/s]\rPrediction:  14%|█▎        | 16/117 [00:16<01:33,  1.08it/s]\rPrediction:  15%|█▍        | 17/117 [00:17<01:30,  1.10it/s]\rPrediction:  15%|█▌        | 18/117 [00:17<01:27,  1.14it/s]\rPrediction:  16%|█▌        | 19/117 [00:18<01:25,  1.15it/s]\rPrediction:  17%|█▋        | 20/117 [00:19<01:24,  1.15it/s]\rPrediction:  18%|█▊        | 21/117 [00:20<01:23,  1.15it/s]\rPrediction:  19%|█▉        | 22/117 [00:21<01:20,  1.18it/s]\rPrediction:  20%|█▉        | 23/117 [00:22<01:20,  1.17it/s]\rPrediction:  21%|██        | 24/117 [00:22<01:18,  1.18it/s]\rPrediction:  21%|██▏       | 25/117 [00:23<01:17,  1.18it/s]\rPrediction:  22%|██▏       | 26/117 [00:24<01:20,  1.14it/s]\rPrediction:  23%|██▎       | 27/117 [00:25<01:17,  1.16it/s]\rPrediction:  24%|██▍       | 28/117 [00:26<01:16,  1.17it/s]\rPrediction:  25%|██▍       | 29/117 [00:27<01:15,  1.17it/s]\rPrediction:  26%|██▌       | 30/117 [00:28<01:13,  1.18it/s]\rPrediction:  26%|██▋       | 31/117 [00:28<01:14,  1.15it/s]\rPrediction:  27%|██▋       | 32/117 [00:29<01:12,  1.17it/s]\rPrediction:  28%|██▊       | 33/117 [00:30<01:11,  1.17it/s]\rPrediction:  29%|██▉       | 34/117 [00:31<01:12,  1.15it/s]\rPrediction:  30%|██▉       | 35/117 [00:32<01:12,  1.14it/s]\rPrediction:  31%|███       | 36/117 [00:33<01:13,  1.11it/s]\rPrediction:  32%|███▏      | 37/117 [00:34<01:11,  1.11it/s]\rPrediction:  32%|███▏      | 38/117 [00:35<01:09,  1.13it/s]\rPrediction:  33%|███▎      | 39/117 [00:35<01:07,  1.15it/s]\rPrediction:  34%|███▍      | 40/117 [00:36<01:07,  1.15it/s]\rPrediction:  35%|███▌      | 41/117 [00:37<01:05,  1.17it/s]\rPrediction:  36%|███▌      | 42/117 [00:38<01:04,  1.16it/s]\rPrediction:  37%|███▋      | 43/117 [00:39<01:03,  1.16it/s]\rPrediction:  38%|███▊      | 44/117 [00:40<01:03,  1.16it/s]\rPrediction:  38%|███▊      | 45/117 [00:41<01:02,  1.15it/s]\rPrediction:  39%|███▉      | 46/117 [00:41<01:00,  1.17it/s]\rPrediction:  40%|████      | 47/117 [00:42<01:00,  1.17it/s]\rPrediction:  41%|████      | 48/117 [00:43<00:58,  1.17it/s]\rPrediction:  42%|████▏     | 49/117 [00:44<00:58,  1.16it/s]\rPrediction:  43%|████▎     | 50/117 [00:45<00:58,  1.15it/s]\rPrediction:  44%|████▎     | 51/117 [00:46<00:56,  1.17it/s]\rPrediction:  44%|████▍     | 52/117 [00:47<00:55,  1.18it/s]\rPrediction:  45%|████▌     | 53/117 [00:47<00:54,  1.17it/s]\rPrediction:  46%|████▌     | 54/117 [00:48<00:53,  1.18it/s]\rPrediction:  47%|████▋     | 55/117 [00:49<00:52,  1.18it/s]\rPrediction:  48%|████▊     | 56/117 [00:50<00:51,  1.18it/s]\rPrediction:  49%|████▊     | 57/117 [00:51<00:50,  1.19it/s]\rPrediction:  50%|████▉     | 58/117 [00:52<00:49,  1.20it/s]\rPrediction:  50%|█████     | 59/117 [00:52<00:47,  1.21it/s]\rPrediction:  51%|█████▏    | 60/117 [00:53<00:47,  1.21it/s]\rPrediction:  52%|█████▏    | 61/117 [00:54<00:46,  1.20it/s]\rPrediction:  53%|█████▎    | 62/117 [00:55<00:46,  1.18it/s]\rPrediction:  54%|█████▍    | 63/117 [00:56<00:45,  1.19it/s]\rPrediction:  55%|█████▍    | 64/117 [00:57<00:44,  1.19it/s]\rPrediction:  56%|█████▌    | 65/117 [00:58<00:43,  1.19it/s]\rPrediction:  56%|█████▋    | 66/117 [00:58<00:42,  1.21it/s]\rPrediction:  57%|█████▋    | 67/117 [00:59<00:41,  1.22it/s]\rPrediction:  58%|█████▊    | 68/117 [01:00<00:40,  1.21it/s]\rPrediction:  59%|█████▉    | 69/117 [01:01<00:39,  1.22it/s]\rPrediction:  60%|█████▉    | 70/117 [01:02<00:38,  1.22it/s]\rPrediction:  61%|██████    | 71/117 [01:02<00:38,  1.19it/s]\rPrediction:  62%|██████▏   | 72/117 [01:03<00:37,  1.19it/s]\rPrediction:  62%|██████▏   | 73/117 [01:04<00:37,  1.18it/s]\rPrediction:  63%|██████▎   | 74/117 [01:05<00:36,  1.17it/s]\rPrediction:  64%|██████▍   | 75/117 [01:06<00:35,  1.18it/s]\rPrediction:  65%|██████▍   | 76/117 [01:07<00:35,  1.17it/s]\rPrediction:  66%|██████▌   | 77/117 [01:08<00:34,  1.17it/s]\rPrediction:  67%|██████▋   | 78/117 [01:08<00:32,  1.18it/s]\rPrediction:  68%|██████▊   | 79/117 [01:09<00:32,  1.18it/s]\rPrediction:  68%|██████▊   | 80/117 [01:10<00:31,  1.19it/s]\rPrediction:  69%|██████▉   | 81/117 [01:11<00:31,  1.16it/s]\rPrediction:  70%|███████   | 82/117 [01:12<00:30,  1.15it/s]\rPrediction:  71%|███████   | 83/117 [01:13<00:28,  1.19it/s]\rPrediction:  72%|███████▏  | 84/117 [01:13<00:27,  1.22it/s]\rPrediction:  73%|███████▎  | 85/117 [01:14<00:26,  1.22it/s]\rPrediction:  74%|███████▎  | 86/117 [01:15<00:25,  1.23it/s]\rPrediction:  74%|███████▍  | 87/117 [01:16<00:24,  1.23it/s]\rPrediction:  75%|███████▌  | 88/117 [01:17<00:23,  1.24it/s]\rPrediction:  76%|███████▌  | 89/117 [01:18<00:22,  1.23it/s]\rPrediction:  77%|███████▋  | 90/117 [01:18<00:21,  1.23it/s]\rPrediction:  78%|███████▊  | 91/117 [01:19<00:20,  1.25it/s]\rPrediction:  79%|███████▊  | 92/117 [01:20<00:19,  1.25it/s]\rPrediction:  79%|███████▉  | 93/117 [01:21<00:18,  1.27it/s]\rPrediction:  80%|████████  | 94/117 [01:21<00:18,  1.27it/s]\rPrediction:  81%|████████  | 95/117 [01:22<00:17,  1.24it/s]\rPrediction:  82%|████████▏ | 96/117 [01:23<00:17,  1.22it/s]\rPrediction:  83%|████████▎ | 97/117 [01:24<00:16,  1.23it/s]\rPrediction:  84%|████████▍ | 98/117 [01:25<00:15,  1.22it/s]\rPrediction:  85%|████████▍ | 99/117 [01:26<00:14,  1.22it/s]\rPrediction:  85%|████████▌ | 100/117 [01:26<00:13,  1.22it/s]\rPrediction:  86%|████████▋ | 101/117 [01:27<00:13,  1.21it/s]\rPrediction:  87%|████████▋ | 102/117 [01:28<00:12,  1.20it/s]\rPrediction:  88%|████████▊ | 103/117 [01:29<00:11,  1.19it/s]\rPrediction:  89%|████████▉ | 104/117 [01:30<00:11,  1.18it/s]\rPrediction:  90%|████████▉ | 105/117 [01:31<00:10,  1.20it/s]\rPrediction:  91%|█████████ | 106/117 [01:31<00:09,  1.21it/s]\rPrediction:  91%|█████████▏| 107/117 [01:32<00:08,  1.20it/s]\rPrediction:  92%|█████████▏| 108/117 [01:33<00:07,  1.17it/s]\rPrediction:  93%|█████████▎| 109/117 [01:34<00:06,  1.16it/s]\rPrediction:  94%|█████████▍| 110/117 [01:35<00:05,  1.17it/s]\rPrediction:  95%|█████████▍| 111/117 [01:36<00:04,  1.21it/s]\rPrediction:  96%|█████████▌| 112/117 [01:37<00:04,  1.20it/s]\rPrediction:  97%|█████████▋| 113/117 [01:37<00:03,  1.22it/s]\rPrediction:  97%|█████████▋| 114/117 [01:38<00:02,  1.22it/s]\rPrediction:  98%|█████████▊| 115/117 [01:39<00:01,  1.22it/s]\rPrediction:  99%|█████████▉| 116/117 [01:40<00:00,  1.20it/s]\rPrediction: 100%|██████████| 117/117 [01:40<00:00,  1.34it/s]\rPrediction: 100%|██████████| 117/117 [01:40<00:00,  1.16it/s]\n",
            "04/16/2022 10:52:32 - INFO - __main__ -     precision = 0.0008143322475570033\n",
            "04/16/2022 10:52:32 - INFO - __main__ -     recall = 0.0021436227224008574\n",
            "04/16/2022 10:52:32 - INFO - __main__ -     f1 = 0.0011802891708468577\n",
            "04/16/2022 10:52:32 - INFO - __main__ -     exact match = 0.0021436227224008574\n",
            "04/16/2022 10:52:32 - INFO - __main__ -     loss = 311.01634098933295\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"/content/FinCausal-2020/pred/\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=pred_new_model\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_predict \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym0xCeZFCumn",
        "outputId": "3bba21b6-2321-43a8-8a39-4d10c60ec5b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "pred_new_model/test_predictions.txt : 933\n",
            "['B-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', '_', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E']\n",
            "Length of eval data: 933\n"
          ]
        }
      ],
      "source": [
        "!python submission.py pred_new_model/test_predictions.txt data/blind_test_2022.csv bert_till2022_trained_output_test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk_AkXMEEbbm"
      },
      "outputs": [],
      "source": [
        "#bert_till2022_trained_output_test.csv is the file for submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6XkFp3yDE5b"
      },
      "outputs": [],
      "source": [
        "!rm -rf pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls6BQnd7CgsO"
      },
      "source": [
        "## Train bert-base-uncased without POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEd3lYaACtj1"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/FinCausal-2020/bert_base_no_pos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfdM-6q6CkeI",
        "outputId": "09d0403d-f66d-45b1-b181-29974f626da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"bert-base-uncased\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=bert_base_no_pos\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run_bert_no_pos.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_train \\\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQh6US3CUGrJ"
      },
      "source": [
        "## Predictions BERT without pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPy9BooHVNjg"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/FinCausal-2020/pred_new_bert_model_no_pos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG42-7BwVhzy",
        "outputId": "a085ee95-18c2-4771-95e1-a5487f1ffdbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_labels: ['_', 'B-C', 'I-C', 'B-E', 'I-E']\n",
            "ADD_POS: False\n",
            "Number of exact matches: 2 out of 933\n",
            "933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "04/16/2022 12:28:44 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "04/16/2022 12:28:44 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/16/2022 12:28:44 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='pred_new_bert_model_no_pos', overwrite_output_dir=False, add_pos=False, do_train=False, do_eval=False, do_predict=True, evaluate_during_training=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir=None, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False)\n",
            "04/16/2022 12:28:44 - INFO - transformers.configuration_utils -   loading configuration file /content/FinCausal-2020/bert_base_no_pos/config.json\n",
            "04/16/2022 12:28:44 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"_\",\n",
            "    \"1\": \"B-C\",\n",
            "    \"2\": \"I-C\",\n",
            "    \"3\": \"B-E\",\n",
            "    \"4\": \"I-E\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-C\": 1,\n",
            "    \"B-E\": 3,\n",
            "    \"I-C\": 2,\n",
            "    \"I-E\": 4,\n",
            "    \"_\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/16/2022 12:28:44 - INFO - transformers.configuration_utils -   loading configuration file /content/FinCausal-2020/bert_base_no_pos/config.json\n",
            "04/16/2022 12:28:44 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"_\",\n",
            "    \"1\": \"B-C\",\n",
            "    \"2\": \"I-C\",\n",
            "    \"3\": \"B-E\",\n",
            "    \"4\": \"I-E\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-C\": 1,\n",
            "    \"B-E\": 3,\n",
            "    \"I-C\": 2,\n",
            "    \"I-E\": 4,\n",
            "    \"_\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/16/2022 12:28:44 - INFO - transformers.tokenization_utils -   Model name '/content/FinCausal-2020/bert_base_no_pos/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/content/FinCausal-2020/bert_base_no_pos/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/16/2022 12:28:44 - INFO - transformers.tokenization_utils -   Didn't find file /content/FinCausal-2020/bert_base_no_pos/added_tokens.json. We won't load it.\n",
            "04/16/2022 12:28:44 - INFO - transformers.tokenization_utils -   loading file /content/FinCausal-2020/bert_base_no_pos/vocab.txt\n",
            "04/16/2022 12:28:44 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/16/2022 12:28:44 - INFO - transformers.tokenization_utils -   loading file /content/FinCausal-2020/bert_base_no_pos/special_tokens_map.json\n",
            "04/16/2022 12:28:44 - INFO - transformers.tokenization_utils -   loading file /content/FinCausal-2020/bert_base_no_pos/tokenizer_config.json\n",
            "04/16/2022 12:28:44 - INFO - transformers.modeling_utils -   loading weights file /content/FinCausal-2020/bert_base_no_pos/pytorch_model.bin\n",
            "04/16/2022 12:28:48 - INFO - transformers.trainer -   You are instantiating a Trainer but wandb is not installed. Install it to use Weights & Biases logging.\n",
            "04/16/2022 12:28:48 - INFO - utils -   Loading features from cached file data/cached_test_BertTokenizer_350\n",
            "04/16/2022 12:28:50 - INFO - transformers.trainer -   ***** Running Prediction *****\n",
            "04/16/2022 12:28:50 - INFO - transformers.trainer -     Num examples = 933\n",
            "04/16/2022 12:28:50 - INFO - transformers.trainer -     Batch size = 8\n",
            "\rPrediction:   0%|          | 0/117 [00:00<?, ?it/s]/content/FinCausal-2020/transformers/modeling_bert.py:1482: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  lengths_ = [torch.nonzero(active_loss[i])[-1].item()+1 for i in range(active_loss.size(0))]\n",
            "\rPrediction:   1%|          | 1/117 [00:00<01:49,  1.06it/s]\rPrediction:   2%|▏         | 2/117 [00:01<01:46,  1.08it/s]\rPrediction:   3%|▎         | 3/117 [00:02<01:44,  1.09it/s]\rPrediction:   3%|▎         | 4/117 [00:03<01:41,  1.11it/s]\rPrediction:   4%|▍         | 5/117 [00:04<01:39,  1.12it/s]\rPrediction:   5%|▌         | 6/117 [00:05<01:36,  1.14it/s]\rPrediction:   6%|▌         | 7/117 [00:06<01:35,  1.15it/s]\rPrediction:   7%|▋         | 8/117 [00:06<01:32,  1.18it/s]\rPrediction:   8%|▊         | 9/117 [00:07<01:31,  1.18it/s]\rPrediction:   9%|▊         | 10/117 [00:09<01:48,  1.02s/it]\rPrediction:   9%|▉         | 11/117 [00:10<01:46,  1.01s/it]\rPrediction:  10%|█         | 12/117 [00:11<01:40,  1.05it/s]\rPrediction:  11%|█         | 13/117 [00:11<01:35,  1.09it/s]\rPrediction:  12%|█▏        | 14/117 [00:12<01:32,  1.11it/s]\rPrediction:  13%|█▎        | 15/117 [00:13<01:30,  1.12it/s]\rPrediction:  14%|█▎        | 16/117 [00:14<01:27,  1.15it/s]\rPrediction:  15%|█▍        | 17/117 [00:15<01:27,  1.14it/s]\rPrediction:  15%|█▌        | 18/117 [00:16<01:25,  1.16it/s]\rPrediction:  16%|█▌        | 19/117 [00:16<01:24,  1.15it/s]\rPrediction:  17%|█▋        | 20/117 [00:17<01:23,  1.16it/s]\rPrediction:  18%|█▊        | 21/117 [00:18<01:23,  1.15it/s]\rPrediction:  19%|█▉        | 22/117 [00:19<01:21,  1.17it/s]\rPrediction:  20%|█▉        | 23/117 [00:20<01:20,  1.17it/s]\rPrediction:  21%|██        | 24/117 [00:21<01:18,  1.19it/s]\rPrediction:  21%|██▏       | 25/117 [00:22<01:16,  1.20it/s]\rPrediction:  22%|██▏       | 26/117 [00:22<01:17,  1.17it/s]\rPrediction:  23%|██▎       | 27/117 [00:23<01:16,  1.18it/s]\rPrediction:  24%|██▍       | 28/117 [00:24<01:17,  1.15it/s]\rPrediction:  25%|██▍       | 29/117 [00:26<01:28,  1.01s/it]\rPrediction:  26%|██▌       | 30/117 [00:27<01:42,  1.18s/it]\rPrediction:  26%|██▋       | 31/117 [00:28<01:43,  1.21s/it]\rPrediction:  27%|██▋       | 32/117 [00:29<01:33,  1.10s/it]\rPrediction:  28%|██▊       | 33/117 [00:30<01:27,  1.04s/it]\rPrediction:  29%|██▉       | 34/117 [00:31<01:23,  1.00s/it]\rPrediction:  30%|██▉       | 35/117 [00:32<01:18,  1.04it/s]\rPrediction:  31%|███       | 36/117 [00:33<01:16,  1.06it/s]\rPrediction:  32%|███▏      | 37/117 [00:34<01:13,  1.09it/s]\rPrediction:  32%|███▏      | 38/117 [00:35<01:11,  1.11it/s]\rPrediction:  33%|███▎      | 39/117 [00:35<01:09,  1.13it/s]\rPrediction:  34%|███▍      | 40/117 [00:36<01:08,  1.13it/s]\rPrediction:  35%|███▌      | 41/117 [00:37<01:06,  1.14it/s]\rPrediction:  36%|███▌      | 42/117 [00:38<01:05,  1.15it/s]\rPrediction:  37%|███▋      | 43/117 [00:39<01:03,  1.16it/s]\rPrediction:  38%|███▊      | 44/117 [00:40<01:02,  1.16it/s]\rPrediction:  38%|███▊      | 45/117 [00:41<01:01,  1.16it/s]\rPrediction:  39%|███▉      | 46/117 [00:41<01:00,  1.17it/s]\rPrediction:  40%|████      | 47/117 [00:42<01:00,  1.16it/s]\rPrediction:  41%|████      | 48/117 [00:43<00:58,  1.18it/s]\rPrediction:  42%|████▏     | 49/117 [00:44<00:58,  1.16it/s]\rPrediction:  43%|████▎     | 50/117 [00:45<00:58,  1.16it/s]\rPrediction:  44%|████▎     | 51/117 [00:46<00:55,  1.18it/s]\rPrediction:  44%|████▍     | 52/117 [00:46<00:55,  1.18it/s]\rPrediction:  45%|████▌     | 53/117 [00:47<00:54,  1.17it/s]\rPrediction:  46%|████▌     | 54/117 [00:48<00:53,  1.18it/s]\rPrediction:  47%|████▋     | 55/117 [00:49<00:51,  1.20it/s]\rPrediction:  48%|████▊     | 56/117 [00:50<00:50,  1.20it/s]\rPrediction:  49%|████▊     | 57/117 [00:51<00:50,  1.18it/s]\rPrediction:  50%|████▉     | 58/117 [00:52<00:49,  1.19it/s]\rPrediction:  50%|█████     | 59/117 [00:52<00:47,  1.22it/s]\rPrediction:  51%|█████▏    | 60/117 [00:53<00:47,  1.20it/s]\rPrediction:  52%|█████▏    | 61/117 [00:54<00:46,  1.19it/s]\rPrediction:  53%|█████▎    | 62/117 [00:56<00:58,  1.06s/it]\rPrediction:  54%|█████▍    | 63/117 [00:57<00:55,  1.02s/it]\rPrediction:  55%|█████▍    | 64/117 [00:57<00:51,  1.03it/s]\rPrediction:  56%|█████▌    | 65/117 [00:58<00:48,  1.08it/s]\rPrediction:  56%|█████▋    | 66/117 [00:59<00:45,  1.13it/s]\rPrediction:  57%|█████▋    | 67/117 [01:00<00:43,  1.16it/s]\rPrediction:  58%|█████▊    | 68/117 [01:01<00:42,  1.16it/s]\rPrediction:  59%|█████▉    | 69/117 [01:01<00:40,  1.18it/s]\rPrediction:  60%|█████▉    | 70/117 [01:02<00:39,  1.18it/s]\rPrediction:  61%|██████    | 71/117 [01:03<00:39,  1.16it/s]\rPrediction:  62%|██████▏   | 72/117 [01:04<00:41,  1.09it/s]\rPrediction:  62%|██████▏   | 73/117 [01:05<00:43,  1.01it/s]\rPrediction:  63%|██████▎   | 74/117 [01:07<00:46,  1.09s/it]\rPrediction:  64%|██████▍   | 75/117 [01:08<00:51,  1.21s/it]\rPrediction:  65%|██████▍   | 76/117 [01:09<00:46,  1.14s/it]\rPrediction:  66%|██████▌   | 77/117 [01:11<00:48,  1.21s/it]\rPrediction:  67%|██████▋   | 78/117 [01:12<00:43,  1.12s/it]\rPrediction:  68%|██████▊   | 79/117 [01:12<00:39,  1.04s/it]\rPrediction:  68%|██████▊   | 80/117 [01:13<00:36,  1.02it/s]\rPrediction:  69%|██████▉   | 81/117 [01:14<00:34,  1.05it/s]\rPrediction:  70%|███████   | 82/117 [01:15<00:32,  1.08it/s]\rPrediction:  71%|███████   | 83/117 [01:16<00:29,  1.14it/s]\rPrediction:  72%|███████▏  | 84/117 [01:16<00:27,  1.18it/s]\rPrediction:  73%|███████▎  | 85/117 [01:17<00:26,  1.20it/s]\rPrediction:  74%|███████▎  | 86/117 [01:18<00:25,  1.21it/s]\rPrediction:  74%|███████▍  | 87/117 [01:19<00:24,  1.21it/s]\rPrediction:  75%|███████▌  | 88/117 [01:20<00:23,  1.23it/s]\rPrediction:  76%|███████▌  | 89/117 [01:21<00:22,  1.24it/s]\rPrediction:  77%|███████▋  | 90/117 [01:21<00:22,  1.23it/s]\rPrediction:  78%|███████▊  | 91/117 [01:22<00:20,  1.24it/s]\rPrediction:  79%|███████▊  | 92/117 [01:23<00:20,  1.23it/s]\rPrediction:  79%|███████▉  | 93/117 [01:24<00:19,  1.26it/s]\rPrediction:  80%|████████  | 94/117 [01:25<00:18,  1.25it/s]\rPrediction:  81%|████████  | 95/117 [01:25<00:18,  1.22it/s]\rPrediction:  82%|████████▏ | 96/117 [01:26<00:17,  1.19it/s]\rPrediction:  83%|████████▎ | 97/117 [01:27<00:16,  1.20it/s]\rPrediction:  84%|████████▍ | 98/117 [01:28<00:15,  1.20it/s]\rPrediction:  85%|████████▍ | 99/117 [01:29<00:14,  1.20it/s]\rPrediction:  85%|████████▌ | 100/117 [01:30<00:14,  1.19it/s]\rPrediction:  86%|████████▋ | 101/117 [01:30<00:13,  1.19it/s]\rPrediction:  87%|████████▋ | 102/117 [01:31<00:12,  1.19it/s]\rPrediction:  88%|████████▊ | 103/117 [01:32<00:11,  1.20it/s]\rPrediction:  89%|████████▉ | 104/117 [01:33<00:10,  1.19it/s]\rPrediction:  90%|████████▉ | 105/117 [01:34<00:09,  1.20it/s]\rPrediction:  91%|█████████ | 106/117 [01:35<00:09,  1.20it/s]\rPrediction:  91%|█████████▏| 107/117 [01:35<00:08,  1.19it/s]\rPrediction:  92%|█████████▏| 108/117 [01:36<00:07,  1.19it/s]\rPrediction:  93%|█████████▎| 109/117 [01:37<00:06,  1.18it/s]\rPrediction:  94%|█████████▍| 110/117 [01:38<00:05,  1.19it/s]\rPrediction:  95%|█████████▍| 111/117 [01:39<00:04,  1.20it/s]\rPrediction:  96%|█████████▌| 112/117 [01:40<00:04,  1.20it/s]\rPrediction:  97%|█████████▋| 113/117 [01:40<00:03,  1.22it/s]\rPrediction:  97%|█████████▋| 114/117 [01:41<00:02,  1.24it/s]\rPrediction:  98%|█████████▊| 115/117 [01:42<00:01,  1.25it/s]\rPrediction:  99%|█████████▉| 116/117 [01:43<00:00,  1.23it/s]\rPrediction: 100%|██████████| 117/117 [01:43<00:00,  1.36it/s]\rPrediction: 100%|██████████| 117/117 [01:43<00:00,  1.13it/s]\n",
            "04/16/2022 12:30:49 - INFO - __main__ -     precision = 0.0008143322475570033\n",
            "04/16/2022 12:30:49 - INFO - __main__ -     recall = 0.0021436227224008574\n",
            "04/16/2022 12:30:49 - INFO - __main__ -     f1 = 0.0011802891708468577\n",
            "04/16/2022 12:30:49 - INFO - __main__ -     exact match = 0.0021436227224008574\n",
            "04/16/2022 12:30:49 - INFO - __main__ -     loss = 311.01634098933295\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"/content/FinCausal-2020/bert_base_no_pos/\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=pred_new_bert_model_no_pos\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run_bert_no_pos.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_predict \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVGLpw_ZWUz_",
        "outputId": "addd9072-3056-4978-b966-de549307a796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file 'submission.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python submission.py pred_new_bert_model_no_pos/test_predictions.txt data/blind_test_2022.csv bert_till2022_trained_output_test_no_pos.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1wAmVI5Do4a"
      },
      "source": [
        "## RoBERTa Large Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HMeKUJjUfWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd984b4-cf8d-481a-d560-fd9437d48225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/FinCausal-2020/roberta_large/’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/FinCausal-2020/roberta_large/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/FinCausal-2020/run_roberta.py\n",
        "!rm /content/FinCausal-2020/nonpos_utils.py\n",
        "!wget https://raw.githubusercontent.com/sohomghosh/FinCausal-2020/master/run_roberta.py -P /content/FinCausal-2020/\n",
        "!wget https://raw.githubusercontent.com/sohomghosh/FinCausal-2020/master/nonpos_utils.py -P /content/FinCausal-2020/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_h5N1aSfR9r",
        "outputId": "c06b01a9-519c-401c-f564-7832a05b35be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-16 13:51:39--  https://raw.githubusercontent.com/sohomghosh/FinCausal-2020/master/run_roberta.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12387 (12K) [text/plain]\n",
            "Saving to: ‘/content/FinCausal-2020/run_roberta.py’\n",
            "\n",
            "\rrun_roberta.py        0%[                    ]       0  --.-KB/s               \rrun_roberta.py      100%[===================>]  12.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-16 13:51:39 (103 MB/s) - ‘/content/FinCausal-2020/run_roberta.py’ saved [12387/12387]\n",
            "\n",
            "--2022-04-16 13:51:39--  https://raw.githubusercontent.com/sohomghosh/FinCausal-2020/master/nonpos_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14219 (14K) [text/plain]\n",
            "Saving to: ‘/content/FinCausal-2020/nonpos_utils.py’\n",
            "\n",
            "nonpos_utils.py     100%[===================>]  13.89K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-04-16 13:51:39 (25.4 MB/s) - ‘/content/FinCausal-2020/nonpos_utils.py’ saved [14219/14219]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDaOMmRWUK_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961ae297-4827-4b2f-dd65-233c84b7a1a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_labels: ['_', 'B-C', 'I-C', 'B-E', 'I-E']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "04/16/2022 13:58:13 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "04/16/2022 13:58:13 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/16/2022 13:58:13 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='roberta_large', overwrite_output_dir=False, add_pos=False, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir=None, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False)\n",
            "04/16/2022 13:58:13 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "04/16/2022 13:58:13 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"_\",\n",
            "    \"1\": \"B-C\",\n",
            "    \"2\": \"I-C\",\n",
            "    \"3\": \"B-E\",\n",
            "    \"4\": \"I-E\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-C\": 1,\n",
            "    \"B-E\": 3,\n",
            "    \"I-C\": 2,\n",
            "    \"I-E\": 4,\n",
            "    \"_\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/16/2022 13:58:13 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "04/16/2022 13:58:13 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "04/16/2022 13:58:13 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "04/16/2022 13:58:20 - INFO - transformers.modeling_utils -   Weights of RobertaForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "04/16/2022 13:58:20 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "04/16/2022 13:58:20 - INFO - nonpos_utils -   Loading features from cached file data/cached_train_RobertaTokenizer_350\n",
            "04/16/2022 13:58:21 - INFO - transformers.trainer -   You are instantiating a Trainer but wandb is not installed. Install it to use Weights & Biases logging.\n",
            "04/16/2022 13:58:21 - INFO - transformers.trainer -   ***** Running training *****\n",
            "04/16/2022 13:58:21 - INFO - transformers.trainer -     Num examples = 2808\n",
            "04/16/2022 13:58:21 - INFO - transformers.trainer -     Num Epochs = 3\n",
            "04/16/2022 13:58:21 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "04/16/2022 13:58:21 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "04/16/2022 13:58:21 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "04/16/2022 13:58:21 - INFO - transformers.trainer -     Total optimization steps = 1053\n",
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "\rIteration:   0%|          | 0/351 [00:00<?, ?it/s]\u001b[A\rIteration:   0%|          | 0/351 [00:00<?, ?it/s]\n",
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"run_roberta.py\", line 328, in <module>\n",
            "    main()\n",
            "  File \"run_roberta.py\", line 246, in main\n",
            "    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
            "  File \"/content/FinCausal-2020/transformers/trainer.py\", line 392, in train\n",
            "    tr_loss += self._training_step(model, inputs, optimizer)\n",
            "  File \"/content/FinCausal-2020/transformers/trainer.py\", line 477, in _training_step\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "TypeError: forward() got an unexpected keyword argument 'pos_tag_ids'\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"roberta-base\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=roberta_large\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run_roberta.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_train \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCU4MMhdXMZs"
      },
      "source": [
        "## Predictions RoBERTa large no pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ahs_WPeXMZs"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/FinCausal-2020/pred_roberta_large_no_pos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyloGlY-XMZt"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"/content/FinCausal-2020/roberta_large/\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=pred_roberta_large_no_pos\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run_roberta.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_predict \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4mCCXkMXMZt"
      },
      "outputs": [],
      "source": [
        "!python submission.py pred_roberta_large_no_pos/test_predictions.txt data/blind_test_2022.csv pred_roberta_large_no_pos_test_output.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain specific train - BERT-SEC no-pos"
      ],
      "metadata": {
        "id": "94glWVqrYaza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/FinCausal-2020/bert-sec/"
      ],
      "metadata": {
        "id": "XBwlN-fcq-9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/nlpaueb/sec-bert-base"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq0m2L-Wr1J7",
        "outputId": "90f01800-2411-4683-c2c7-c457fe58965b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sec-bert-base'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 121 (delta 8), reused 0 (delta 0), pack-reused 106\u001b[K\n",
            "Receiving objects: 100% (121/121), 123.94 KiB | 2.10 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"sec-bert-base\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=bert-sec\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run_domain_specific.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_train \\\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR7ChZr8rGGG",
        "outputId": "5a858140-c6ef-460d-dfdf-f4d2dc717e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfgVTBm54D_1"
      },
      "source": [
        "## Predictions Domain specific train - BERT-SEC no-pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5QsOpWO4D_1"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/FinCausal-2020/pred_bert_sec_no_pos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6I0fVA64D_2"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"/content/FinCausal-2020/bert-sec/\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=pred_bert_sec_no_pos\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run_domain_specific.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_predict \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErX2UOAa4D_2"
      },
      "outputs": [],
      "source": [
        "!python submission.py pred_bert_sec_no_pos/test_predictions.txt data/blind_test_2022.csv pred_bert_sec_no_pos_test_output.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain specific train - BERT-SEC no-pos"
      ],
      "metadata": {
        "id": "VJK7RXZD4x0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/FinCausal-2020/finbert-cau/"
      ],
      "metadata": {
        "id": "PrB5EHVo4_v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/ProsusAI/finbert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6FfcWuyreZR",
        "outputId": "0be7dc82-3238-4947-d4c8-b9184cadaac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'finbert'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 49 (delta 23), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"finbert\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=finbert-cau\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run_domain_specific.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_train \\\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAN1FWO15F6y",
        "outputId": "f081cf47-954c-436c-ed8b-da7550cc1a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwDNuBr25mub"
      },
      "source": [
        "## Predictions Domain specific train - FinBERT no-pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_fl4wWg5muc"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/FinCausal-2020/pred_finbert_no_pos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H90SIIN5muc"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"/content/FinCausal-2020/finbert-cau/\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=pred_finbert_no_pos\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run_domain_specific.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_predict \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8fJhchr5mud"
      },
      "outputs": [],
      "source": [
        "!python submission.py pred_finbert_no_pos/test_predictions.txt data/blind_test_2022.csv pred_finbert_no_pos_test_output.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrain previously trained Fincausal Model"
      ],
      "metadata": {
        "id": "j72dr5NB6jeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download and upload the file separately from https://drive.google.com/file/d/1omc-hy4uAb1JaeVrNQvbvGOQ3tZga7C3/view\n",
        "!unzip /content/FinCausal-2020/eval_bio.zip -d /content/FinCausal-2020/"
      ],
      "metadata": {
        "id": "qVuWNjLW6opi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"eval_bio\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=fincau2020_retrain\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run_domain_specific.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_train \\\n"
      ],
      "metadata": {
        "id": "O64Ye92E6_tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epIuC4tl7sQC"
      },
      "source": [
        "## Predictions Domain specific train - FinCausal-2020 extend no-pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjN0KWqe7sQD"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/FinCausal-2020/pred_fcausal2020_ex_no_pos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn_dyFzG7sQD"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "export MAX_LENGTH=350\n",
        "export BERT_MODEL=\"/content/FinCausal-2020/fincau2020_retrain/\"\n",
        "\n",
        "export DATA_DIR=data\n",
        "export OUTPUT_DIR=pred_fcausal2020_ex_no_pos\n",
        "export SEED=1\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run_domain_specific.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--labels $DATA_DIR/labels.txt \\\n",
        "--model_name_or_path $BERT_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--seed $SEED \\\n",
        "--do_predict \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8f16g0o7sQE"
      },
      "outputs": [],
      "source": [
        "!python submission.py pred_fcausal2020_ex_no_pos/test_predictions.txt data/blind_test_2022.csv pred_fcausal2020_ex_no_pos_test_output.csv"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "iMuNwwQv6Pyf",
        "6dDoWLjv7CkY",
        "wStjhvs-7Fsb"
      ],
      "name": "RE-TRAIN FinCausal_2020_best_solution_score_2022data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6SHZu3CtEA95jA6udq5zR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}